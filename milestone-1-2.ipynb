{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f93e01-4832-49f9-9a6c-14774cf51622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c1074-d403-4a87-a919-b930b44e3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_df = pd.read_csv('news-commentary-v15.en-zh.tsv',sep = '\\t', header=None).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2201717d-2092-4a3e-91d8-465531044965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf61a81713b448f8018e21773d94448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446aa2f3eb2f42ebb8b4841736bd3804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7d821105fb491cad556c637f2a9e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/807k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8cc3bf23134a8fa567f90450c345dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54b3e3ae135468fbc5a8f2d3271d852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31801\\miniconda3\\envs\\csci-444-project\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae895a45cbfa4cea97cb12fbdd2a9180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ca44a1ede14f2dbe07ee9e15e918c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234a3178-dd27-42b0-b453-7b4b532a6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for demonstration\n",
    "source_sentences = [\"你好\", \"谢谢\"]\n",
    "target_sentences = [\"Hello\", \"Thank you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf149cca-14c0-4983-9a89-ce4eec2395d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_anchor_points = [\"你好\", \"学习\"]  # Chinese anchor words\n",
    "target_anchor_points = [\"Hello\", \"study\"]  # English equivalent anchor words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602e6da4-1bf0-4021-b1ca-3c0fc3701f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_anchor_points(text, anchor_points):\n",
    "    for anchor in anchor_points:\n",
    "        text = text.replace(anchor, f\"<<{anchor}>>\")\n",
    "    return text\n",
    "\n",
    "source_sentences = [embed_anchor_points(sent, source_anchor_points) for sent in source_sentences]\n",
    "target_sentences = [embed_anchor_points(sent, target_anchor_points) for sent in target_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6007dd-be28-4e0b-aac4-7d6be884f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_texts, target_texts, tokenizer):\n",
    "        self.source_texts = source_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.source_texts[idx]\n",
    "        target = self.target_texts[idx]\n",
    "        source_enc = self.tokenizer(source, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "        target_enc = self.tokenizer(target, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b80ca610-a093-4650-af59-264f60adaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataset and dataloader\n",
    "dataset = TranslationDataset(source_sentences, target_sentences, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fa1df3-42c7-4531-ad8b-ac6ac82913ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss_function(output_logits, target_ids, anchor_mask):\n",
    "    \"\"\"\n",
    "    Compute custom loss with additional emphasis on anchor points.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_logits: Model's output logits\n",
    "    - target_ids: Actual target token IDs\n",
    "    - anchor_mask: Mask indicating anchor positions in the target sequence\n",
    "    \n",
    "    Returns:\n",
    "    - Loss with penalties on anchor errors\n",
    "    \"\"\"\n",
    "    loss = F.cross_entropy(output_logits.view(-1, output_logits.size(-1)), target_ids.view(-1), reduction=\"none\")\n",
    "    \n",
    "    # Penalty: amplify loss for anchor point errors\n",
    "    loss = loss.view(target_ids.shape) * (1 + anchor_mask * 2)  # Triple the loss weight for anchors\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae158cdd-1e74-47ad-8975-061742dda313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anchor_mask(tokenizer, sentences, anchor_points):\n",
    "    \"\"\"\n",
    "    Create a mask to identify anchor point tokens in the target sequence.\n",
    "    \n",
    "    Parameters:\n",
    "    - tokenizer: The tokenizer for the model\n",
    "    - sentences: List of target sentences\n",
    "    - anchor_points: List of anchor words\n",
    "    \n",
    "    Returns:\n",
    "    - Mask tensor indicating anchor tokens\n",
    "    \"\"\"\n",
    "    anchor_mask = []\n",
    "    for sentence in sentences:\n",
    "        mask = [1 if word in anchor_points else 0 for word in tokenizer.tokenize(sentence)]\n",
    "        mask = mask + [0] * (64 - len(mask))  # pad to max length if needed\n",
    "        anchor_mask.append(mask[:64])\n",
    "    return torch.tensor(anchor_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33d9014-b928-481d-834f-4aba0c86d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dff0d8b-681d-4811-a384-99cc2f62261a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 7.913210868835449\n",
      "Epoch 2/3, Loss: 7.041543006896973\n",
      "Epoch 3/3, Loss: 5.931280136108398\n"
     ]
    }
   ],
   "source": [
    "# Training loop with custom loss function\n",
    "model.train()\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        # Generate anchor mask\n",
    "        anchor_mask = create_anchor_mask(tokenizer, target_sentences, target_anchor_points).to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Calculate custom loss with anchor points\n",
    "        loss = custom_loss_function(outputs.logits, labels, anchor_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f9b77f-7980-4290-98ee-5c39a2d6dc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese: 你好，欢迎来到这里学习\n",
      "English Translation: Hello. Welcome to school.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "new_sentence = \"你好，欢迎来到这里学习\"  # Test sentence with anchor points\n",
    "input_ids = tokenizer.encode(new_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Generate translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids)\n",
    "    translation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Chinese: {new_sentence}\")\n",
    "print(f\"English Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f584487-97a8-4874-a2b7-67606c802110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32ccbd-512d-4a92-92aa-724827e19a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced941d2-96f0-4d8e-83cf-a07bc1e444e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
