{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f68ba62e-eb8d-4d18-a5c5-fe5aa808819e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import stanza\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('zh') \n",
    "nlp = stanza.Pipeline('zh', processors='tokenize')\n",
    "\n",
    "# Get the set of stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(\n",
    "    {'cent', 'href=', 'http', 'says', 'told', 'year', 'ago', 'yesterday', 'since', 'last', 'past', 'next',\n",
    "     'said', 'almost', 'within', 'would', 'nearly', 'years', 'months', 'according', 'compared', 'go', 'also', \n",
    "     \"n't\"})  \n",
    "punctuation_set = set(punctuation)\n",
    "punctuation_set.update({\"’\", \"’\", '”', \"''\", \"“\", \"'s\", '--', 'b', '/b', '/strong', '–', '—'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e89c0453ef70e40",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "class ParallelSentence: \n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class AnchorWord:\n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class ParallelCorpus: \n",
    "    def __init__(self):\n",
    "        self.parallel_sentences = []\n",
    "        self.multi_grams_to_consider = []\n",
    "        self.anchor_words = {}\n",
    "        \n",
    "    def load_parallel_sentences(self, data_source):\n",
    "        parallel_sentences = []\n",
    "        for file in os.listdir(data_source):\n",
    "            file_path = os.path.join(data_source, file)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as data_file:\n",
    "                reader = csv.reader(data_file, delimiter=';')\n",
    "                for row in reader:\n",
    "                    if len(row) < 7: continue   # escape bad data\n",
    "                    english_content = row[5]    # get english sentences\n",
    "                    chinese_content = row[6]    # get chinese sentences\n",
    "        \n",
    "                    # break apart sentence content by @ delimiter\n",
    "                    english_sentences = english_content.split('@')  \n",
    "                    chinese_sentences = chinese_content.split('@')\n",
    "                    \n",
    "                    for english_sentence, chinese_sentence in zip(english_sentences, chinese_sentences):\n",
    "                        clean_english_sentence = english_sentence.strip()\n",
    "                        \n",
    "                        # Process the Chinese sentence with Stanza\n",
    "                        doc = nlp(chinese_sentence)  \n",
    "                        chinese_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "                        clean_chinese_sentence = \" \".join(chinese_tokens)\n",
    "                        \n",
    "                        parallel_sentences.append(ParallelSentence(clean_english_sentence, clean_chinese_sentence))\n",
    "                        \n",
    "        self.parallel_sentences = parallel_sentences\n",
    "    \n",
    "    def generate_multi_grams(self):\n",
    "        bigrams = self.extract_ngram_counts(n=2).most_common()[:5000]\n",
    "        trigrams = self.extract_ngram_counts(n=3).most_common()[:3000]\n",
    "        quadgrams = self.extract_ngram_counts(n=4).most_common()[:1000]\n",
    "        \n",
    "        multi_grams_to_consider = set()\n",
    "        # Add multi-word terms from quad_grams_to_consider\n",
    "        for quad_gram in quadgrams:\n",
    "            multi_word_term = '_'.join(quad_gram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from trigrams_to_consider\n",
    "        for trigram in trigrams:\n",
    "            multi_word_term = '_'.join(trigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from bigrams_to_consider\n",
    "        for bigram in bigrams:\n",
    "            multi_word_term = '_'.join(bigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        self.multi_grams_to_consider = multi_grams_to_consider\n",
    "        \n",
    "    @staticmethod\n",
    "    def refactor_sentence_with_multiword_term(sentence, multi_word_terms):\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split(' ')\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            found = False\n",
    "            \n",
    "            # Check for quadgrams (4-word sequences)\n",
    "            for length in range(4, 1, -1):  # Check for quadgram to bigram\n",
    "                if i + length <= len(words):\n",
    "                    multi_word_candidate = '_'.join(words[i:i+length]).lower()\n",
    "                    if multi_word_candidate in multi_word_terms:\n",
    "                        # If a match is found, replace the words with the multi-word term\n",
    "                        modified_sentence.append(multi_word_candidate)\n",
    "                        i += length\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                # If no match is found, just add the word as is\n",
    "                modified_sentence.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        # Return the modified sentence as a string\n",
    "        return ' '.join(modified_sentence)\n",
    "    \n",
    "    def extract_ngram_counts(self, n):\n",
    "        ngram_counts = Counter()\n",
    "        for parallel_sentence in self.parallel_sentences:\n",
    "            tokens = nltk.word_tokenize(parallel_sentence.en)\n",
    "            # Filter out stopwords, punctuation, and numbers\n",
    "            filtered_tokens = [token.lower() for token in tokens \n",
    "                               if token.lower() not in stop_words \n",
    "                               and token not in punctuation_set \n",
    "                               and not token.isdigit()] \n",
    "    \n",
    "            # Generate n-grams for the filtered tokens\n",
    "            ngram_list = ngrams(filtered_tokens, n)\n",
    "            # Count the frequency of each n-gram\n",
    "            ngram_counts.update(ngram_list)\n",
    "        return ngram_counts\n",
    "    \n",
    "    def format_parallel_sentences_for_awesome_align(self):\n",
    "        with open(\"zhen.src-tgt\", \"w\") as f:\n",
    "            for parallel_sentence in self.parallel_sentences:\n",
    "                modified_sentence = self.refactor_sentence_with_multiword_term(parallel_sentence.en, self.multi_grams_to_consider)\n",
    "                f.write(f\"{modified_sentence} ||| {parallel_sentence.zh}\\n\")\n",
    "    \n",
    "    def build_anchor_words_from_awesome_align_output(self, alignments_path):\n",
    "        anchor_words = []\n",
    "        with open(alignments_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                alignment_pairs = line.strip().split(' ')\n",
    "                for index, pair in enumerate(alignment_pairs):\n",
    "                    en_entry, zh_entry = pair.split('<sep>')[0], pair.split('<sep>')[1]\n",
    "                    if en_entry not in self.multi_grams_to_consider: continue\n",
    "                    # Clean the English entry\n",
    "                    cleaned_en_entry = re.sub(r'[^a-zA-Z_]', '', en_entry)\n",
    "                    \n",
    "                    # Append only if conditions are met\n",
    "                    if cleaned_en_entry:\n",
    "                        if anchor_words and anchor_words[len(anchor_words)-1].en == cleaned_en_entry:\n",
    "                            if zh_entry not in anchor_words[len(anchor_words)-1].zh:\n",
    "                                anchor_words[len(anchor_words)-1].zh += zh_entry\n",
    "                        else:\n",
    "                            anchor_words.append(AnchorWord(cleaned_en_entry, zh_entry))\n",
    "                            \n",
    "        unique_anchors = set(AnchorWord(anchor.en, anchor.zh) for anchor in anchor_words)\n",
    "        \n",
    "        # Step 1: Count frequencies of `zh` entries for each `en`\n",
    "        anchor_freq = defaultdict(Counter)\n",
    "    \n",
    "        for anchor in unique_anchors:\n",
    "            anchor_freq[anchor.en][anchor.zh] += 1\n",
    "        \n",
    "        # Step 2: Select the most frequent `zh` entry for each `en`\n",
    "        filtered_alignments = []\n",
    "        for en, zh_counter in anchor_freq.items():\n",
    "            most_frequent_zh = zh_counter.most_common(1)[0][0]  # Get the most frequent `zh`\n",
    "            filtered_alignments.append(AnchorWord(en, most_frequent_zh))\n",
    "        \n",
    "        # Step 3: Sort alphabetically by `en`\n",
    "        sorted_filtered_anchors = sorted(filtered_alignments, key=lambda anchor: anchor.en)\n",
    "        \n",
    "        # Step 4: Write to file\n",
    "        with open('possible-anchors.txt', 'w') as file:\n",
    "            for alignment in sorted_filtered_anchors:\n",
    "                file.write(f\"{alignment.en} {alignment.zh}\\n\")\n",
    "    \n",
    "    def load_sorted_anchors(self, anchor_path):\n",
    "        anchors = set()\n",
    "        with open(anchor_path, 'r') as file:\n",
    "            for line in file: \n",
    "                alignment = line.strip().split(' ')\n",
    "                en = alignment[0].replace('_', ' ')\n",
    "                zh = alignment[1] \n",
    "                anchors.add((en, zh))  # Store as a tuple for paired lookup\n",
    "        self.anchor_words = anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed50fde78b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus = ParallelCorpus()  # Initialize Corpus Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae262f0a64e3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus.load_parallel_sentences(data_source='./FTIE/')  # Load parallel sentences from data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d953742ad51578",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus.generate_multi_grams()  # Generate Multi grams e.g Asian Financial Crisis -> asian_financial_crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cbeea37e3df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus.format_parallel_sentences_for_awesome_align() # Format English Sentence With Multi Grams \n",
    "# Prepare a data source for awesome align \n",
    "\n",
    "# DATA_FILE=./zhen.src-tgt\n",
    "# MODEL_NAME_OR_PATH=./model_without_co\n",
    "# OUTPUT_FILE=./output.txt\n",
    "# OUTPUT_WORDS=./alignments.txt\n",
    "# OUTPUT_PROB=./alignments-prob.txt\n",
    "# \n",
    "# CUDA_VISIBLE_DEVICES=0 awesome-align \\\n",
    "#     --output_file=$OUTPUT_FILE \\\n",
    "#     --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "#     --data_file=$DATA_FILE \\\n",
    "#     --extraction 'softmax' \\\n",
    "#     --batch_size 32 \\\n",
    "#     --num_workers 0 \\\n",
    "#     --output_word_file=$OUTPUT_WORDS \\\n",
    "#     --output_prob_file=$OUTPUT_PROB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a996a8a95010c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus.build_anchor_words_from_awesome_align_output('./alignments.txt')    # Generate Possible Anchor Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9f9e66c29a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus.load_sorted_anchors('./final_anchors.txt')  # Load Final and Verified Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b082515fe9f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How would the original model translate these anchor words? \n",
    "# def translate_anchor_words(src_lang, tgt_lang, output_file):\n",
    "#     # Set the source and target languages\n",
    "#     tokenizer.src_lang = src_lang\n",
    "#     tokenizer.tgt_lang = tgt_lang\n",
    "#     forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]  # Ensure the target language is correct\n",
    "# \n",
    "#     # Translate and save results\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for index, pair in enumerate(parallel_corpus.anchor_words):\n",
    "#             # Select source and target based on direction\n",
    "#             source_anchor = pair.zh if src_lang == \"zh_CN\" else pair.en\n",
    "#             target_anchor = pair.en if src_lang == \"zh_CN\" else pair.zh\n",
    "# \n",
    "#             # Tokenize the input text\n",
    "#             inputs = tokenizer(source_anchor, return_tensors=\"pt\")\n",
    "#             # Generate translation with forced BOS token for the target language\n",
    "#             translated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "#             # Decode the translated tokens\n",
    "#             translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "# \n",
    "#             # Save the result in the text file\n",
    "#             f.write(f\"{source_anchor}; {target_anchor}; {translation.lower()}\\n\")\n",
    "# \n",
    "#             if index % 100 == 0:\n",
    "#                 print(f\"Done translating {index} / {len(parallel_corpus.anchor_words)}\")\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd3eea578b97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translate English to Chinese\n",
    "# translate_anchor_words(\n",
    "#     src_lang=\"en_XX\",\n",
    "#     tgt_lang=\"zh_CN\",\n",
    "#     output_file=\"unmodified_en-zh-translated_anchor_words.txt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fdc7447b207a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate_anchor_words(\n",
    "#     src_lang=\"zh_CN\",\n",
    "#     tgt_lang=\"en_XX\",\n",
    "#     output_file=\"unmodified_zh-en-translated_anchor_words.txt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85df343acb503d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_zh-en-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         zh_anchor = items[0].strip()\n",
    "#         en_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#         \n",
    "#         if translation == en_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on Chinese Anchor Words (zh->en):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c5e0d1b326585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_en-zh-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         en_anchor = items[0].strip()\n",
    "#         zh_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#             \n",
    "#         if translation == zh_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on English Anchor Words (en->zh):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee50f476ebc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_words_dict = {en: zh for en, zh in parallel_corpus.anchor_words}\n",
    "\n",
    "def refactor_sentence_with_anchors(en_sentence, chinese_sentence, anchor_words):\n",
    "    # Tokenize the sentence into words\n",
    "    words = en_sentence.split(' ')\n",
    "    modified_sentence = []\n",
    "    i = 0\n",
    "    refactored_chinese_sentence = chinese_sentence.replace(' ', '')\n",
    "    while i < len(words):\n",
    "        found = False\n",
    "        \n",
    "        # Check for multi-word anchor terms in English\n",
    "        for length in range(4, 1, -1):  # Check from 4 words (quadgram) to 2 words (bigram)\n",
    "            if i + length <= len(words):\n",
    "                multi_word_candidate = ' '.join(words[i:i+length]).lower()  # Make sure we match underscore-separated terms\n",
    "                # Iterate over the anchor words and check the English part of the pair\n",
    "                for en_term, zh_term in anchor_words:\n",
    "                    if multi_word_candidate == en_term:\n",
    "                        modified_sentence.append(f\"<{multi_word_candidate.replace(' ', '_')}>\")  # Replace with English term\n",
    "                        i += length  # Skip the words that are part of the multi-word term\n",
    "                        found = True\n",
    "                        refactored_chinese_sentence = refactored_chinese_sentence.replace(anchor_words_dict[en_term], '<'+zh_term+'>')\n",
    "                        \n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "        \n",
    "        if not found:\n",
    "            # If no multi-word term is found, just add the current word\n",
    "            modified_sentence.append(words[i])\n",
    "            i += 1\n",
    "    \n",
    "    # Return the modified sentence as a string\n",
    "    return ' '.join(modified_sentence), refactored_chinese_sentence\n",
    "\n",
    "\n",
    "# Example to refactor both English and Chinese sentences\n",
    "refactored_parallel_sentences = []\n",
    "for index, parallel_sentence in enumerate(parallel_corpus.parallel_sentences):\n",
    "    # Refactor the English sentence with anchor words\n",
    "    modified_english_sentence, modified_chinese_sentence = refactor_sentence_with_anchors(parallel_sentence.en, parallel_sentence.zh, parallel_corpus.anchor_words)\n",
    "\n",
    "    # Append the refactored sentence pair to the list\n",
    "    refactored_parallel_sentences.append(ParallelSentence(modified_english_sentence, modified_chinese_sentence))\n",
    "    if index % 1000 == 0: \n",
    "        print(\"Done refactoring\", index, \"out of\", len(parallel_corpus.parallel_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a9282edd086b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('refactored_parallel_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for pair in refactored_parallel_sentences:\n",
    "        f.write(f\"{pair.en} ; {pair.zh}\\n\")\n",
    "        \n",
    "print(\"Refactored sentences saved to 'refactored_parallel_sentences.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87b2787517f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus = ParallelCorpus()\n",
    "parallel_corpus.load_sorted_anchors('./final_anchors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993fe0b6fe3f459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def save_translation_to_csv(ps, output_file):\n",
    "    # Extract English and Chinese texts from the input object\n",
    "    english_text = ps.en\n",
    "    chinese_text = ps.zh\n",
    "\n",
    "    # Create a list of dictionaries containing the data to be saved in the CSV file\n",
    "    data = [{'zh': chinese_text, 'en': english_text}]\n",
    "\n",
    "    # Define the CSV file headers\n",
    "    fieldnames = ['zh', 'en']\n",
    "\n",
    "    # Check if the CSV file already exists and is non-empty\n",
    "    file_exists = os.path.isfile(output_file) and os.path.getsize(output_file) > 0\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is newly created or empty\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the data row\n",
    "        writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e021026b4bfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parallel_sentence in refactored_parallel_sentences:\n",
    "    save_translation_to_csv(parallel_sentence, 'training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda039d2c0aa2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_be_added = []\n",
    "for (en_anchor, zh_anchor) in parallel_corpus.anchor_words:\n",
    "    tokens_to_be_added.append('<'+en_anchor.replace(' ', '_')+'>')\n",
    "    tokens_to_be_added.append('<'+zh_anchor+'>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174ece9bbe598ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c08cb23518531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3c1467e5a7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './training_data.csv'\n",
    "dataset1 = pd.read_csv(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef04154912dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1a268d578f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset1[:175000] \n",
    "validation_dataset = dataset1[175000:200000] \n",
    "\n",
    "train_dataset_hf = Dataset.from_pandas(train_dataset)\n",
    "validation_dataset_hf = Dataset.from_pandas(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87aabc16709a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add custom tokens and resize model embeddings\n",
    "tokenizer.add_tokens(tokens_to_be_added)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179939cbc08dea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"zh\"], max_length=1024, padding=\"max_length\",truncation=True)\n",
    "\n",
    "    target_encodings = tokenizer(example_batch[\"en\"], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "           \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "           \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "train_dataset_tf = train_dataset_hf.map(convert_examples_to_features, batched=True, remove_columns=[\"zh\",\"en\"])\n",
    "val_dataset_tf = validation_dataset_hf.map(convert_examples_to_features, batched=True, remove_columns=[\"zh\",\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460199c4d6a45719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49369d74ed42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8df5f5d313b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer,TrainingArguments, Trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='mbartTrans',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='no',\n",
    "    eval_steps=2000,\n",
    "    logging_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    fp16=False,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adafactor\",\n",
    "    no_cuda=True  # Forces CPU training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f883b0e387f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476111bcba9decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model=model, args=training_args, tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=train_dataset_tf,\n",
    "                        eval_dataset=val_dataset_tf)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8027c27505444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d0c02-5c97-45a6-a254-e0d734501f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
