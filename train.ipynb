{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:55:58.493445Z",
     "start_time": "2024-11-26T23:55:55.005558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import csv\n",
    "\n",
    "import stanza\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('zh') \n",
    "nlp = stanza.Pipeline('zh', processors='tokenize')\n",
    "\n",
    "# Get the set of stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(\n",
    "    {'cent', 'href=', 'http', 'says', 'told', 'year', 'ago', 'yesterday', 'since', 'last', 'past', 'next',\n",
    "     'said', 'almost', 'within', 'would', 'nearly', 'years', 'months', 'according', 'compared', 'go', 'also', \n",
    "     \"n't\"})  \n",
    "punctuation_set = set(punctuation)\n",
    "punctuation_set.update({\"’\", \"’\", '”', \"''\", \"“\", \"'s\", '--', 'b', '/b', '/strong', '–', '—'})"
   ],
   "id": "a90636181e9940f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 51.1MB/s]                    \n",
      "2024-11-26 15:55:55 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 15:55:55 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 15:55:55 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2024-11-26 15:55:56 INFO: File exists: /Users/vnnsnnt/stanza_resources/zh-hans/default.zip\n",
      "2024-11-26 15:55:58 INFO: Finished downloading models and saved to /Users/vnnsnnt/stanza_resources\n",
      "2024-11-26 15:55:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 74.8MB/s]                    \n",
      "2024-11-26 15:55:58 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 15:55:58 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 15:55:58 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsdsimp |\n",
      "=======================\n",
      "\n",
      "2024-11-26 15:55:58 INFO: Using device: cpu\n",
      "2024-11-26 15:55:58 INFO: Loading: tokenize\n",
      "/Users/vnnsnnt/Workspace/Project-NLP/venv/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-26 15:55:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:34:47.200539Z",
     "start_time": "2024-11-28T01:34:47.187373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Structures\n",
    "class ParallelSentence: \n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class AnchorWord:\n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class ParallelCorpus: \n",
    "    def __init__(self):\n",
    "        self.parallel_sentences = []\n",
    "        self.multi_grams_to_consider = []\n",
    "        self.anchor_words = {}\n",
    "        \n",
    "    def load_parallel_sentences(self, data_source):\n",
    "        parallel_sentences = []\n",
    "        for file in os.listdir(data_source):\n",
    "            file_path = os.path.join(data_source, file)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as data_file:\n",
    "                reader = csv.reader(data_file, delimiter=';')\n",
    "                for row in reader:\n",
    "                    if len(row) < 7: continue   # escape bad data\n",
    "                    english_content = row[5]    # get english sentences\n",
    "                    chinese_content = row[6]    # get chinese sentences\n",
    "        \n",
    "                    # break apart sentence content by @ delimiter\n",
    "                    english_sentences = english_content.split('@')  \n",
    "                    chinese_sentences = chinese_content.split('@')\n",
    "                    \n",
    "                    for english_sentence, chinese_sentence in zip(english_sentences, chinese_sentences):\n",
    "                        clean_english_sentence = english_sentence.strip()\n",
    "                        \n",
    "                        # Process the Chinese sentence with Stanza\n",
    "                        doc = nlp(chinese_sentence)  \n",
    "                        chinese_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "                        clean_chinese_sentence = \" \".join(chinese_tokens)\n",
    "                        \n",
    "                        parallel_sentences.append(ParallelSentence(clean_english_sentence, clean_chinese_sentence))\n",
    "                        \n",
    "        self.parallel_sentences = parallel_sentences\n",
    "    \n",
    "    def generate_multi_grams(self):\n",
    "        bigrams = self.extract_ngram_counts(n=2).most_common()[:5000]\n",
    "        trigrams = self.extract_ngram_counts(n=3).most_common()[:3000]\n",
    "        quadgrams = self.extract_ngram_counts(n=4).most_common()[:1000]\n",
    "        \n",
    "        multi_grams_to_consider = set()\n",
    "        # Add multi-word terms from quad_grams_to_consider\n",
    "        for quad_gram in quadgrams:\n",
    "            multi_word_term = '_'.join(quad_gram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from trigrams_to_consider\n",
    "        for trigram in trigrams:\n",
    "            multi_word_term = '_'.join(trigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from bigrams_to_consider\n",
    "        for bigram in bigrams:\n",
    "            multi_word_term = '_'.join(bigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        self.multi_grams_to_consider = multi_grams_to_consider\n",
    "        \n",
    "    @staticmethod\n",
    "    def refactor_sentence_with_multiword_term(sentence, multi_word_terms):\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split(' ')\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            found = False\n",
    "            \n",
    "            # Check for quadgrams (4-word sequences)\n",
    "            for length in range(4, 1, -1):  # Check for quadgram to bigram\n",
    "                if i + length <= len(words):\n",
    "                    multi_word_candidate = '_'.join(words[i:i+length]).lower()\n",
    "                    if multi_word_candidate in multi_word_terms:\n",
    "                        # If a match is found, replace the words with the multi-word term\n",
    "                        modified_sentence.append(multi_word_candidate)\n",
    "                        i += length\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                # If no match is found, just add the word as is\n",
    "                modified_sentence.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        # Return the modified sentence as a string\n",
    "        return ' '.join(modified_sentence)\n",
    "    \n",
    "    def extract_ngram_counts(self, n):\n",
    "        ngram_counts = Counter()\n",
    "        for parallel_sentence in self.parallel_sentences:\n",
    "            tokens = nltk.word_tokenize(parallel_sentence.en)\n",
    "            # Filter out stopwords, punctuation, and numbers\n",
    "            filtered_tokens = [token.lower() for token in tokens \n",
    "                               if token.lower() not in stop_words \n",
    "                               and token not in punctuation_set \n",
    "                               and not token.isdigit()] \n",
    "    \n",
    "            # Generate n-grams for the filtered tokens\n",
    "            ngram_list = ngrams(filtered_tokens, n)\n",
    "            # Count the frequency of each n-gram\n",
    "            ngram_counts.update(ngram_list)\n",
    "        return ngram_counts\n",
    "    \n",
    "    def format_parallel_sentences_for_awesome_align(self):\n",
    "        with open(\"zhen.src-tgt\", \"w\") as f:\n",
    "            for parallel_sentence in self.parallel_sentences:\n",
    "                modified_sentence = self.refactor_sentence_with_multiword_term(parallel_sentence.en, self.multi_grams_to_consider)\n",
    "                f.write(f\"{modified_sentence} ||| {parallel_sentence.zh}\\n\")\n",
    "    \n",
    "    def build_anchor_words_from_awesome_align_output(self, alignments_path):\n",
    "        anchor_words = []\n",
    "        with open(alignments_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                alignment_pairs = line.strip().split(' ')\n",
    "                for index, pair in enumerate(alignment_pairs):\n",
    "                    en_entry, zh_entry = pair.split('<sep>')[0], pair.split('<sep>')[1]\n",
    "                    if en_entry not in self.multi_grams_to_consider: continue\n",
    "                    # Clean the English entry\n",
    "                    cleaned_en_entry = re.sub(r'[^a-zA-Z_]', '', en_entry)\n",
    "                    \n",
    "                    # Append only if conditions are met\n",
    "                    if cleaned_en_entry:\n",
    "                        if anchor_words and anchor_words[len(anchor_words)-1].en == cleaned_en_entry:\n",
    "                            if zh_entry not in anchor_words[len(anchor_words)-1].zh:\n",
    "                                anchor_words[len(anchor_words)-1].zh += zh_entry\n",
    "                        else:\n",
    "                            anchor_words.append(AnchorWord(cleaned_en_entry, zh_entry))\n",
    "                            \n",
    "        unique_anchors = set(AnchorWord(anchor.en, anchor.zh) for anchor in anchor_words)\n",
    "        \n",
    "        # Step 1: Count frequencies of `zh` entries for each `en`\n",
    "        anchor_freq = defaultdict(Counter)\n",
    "    \n",
    "        for anchor in unique_anchors:\n",
    "            anchor_freq[anchor.en][anchor.zh] += 1\n",
    "        \n",
    "        # Step 2: Select the most frequent `zh` entry for each `en`\n",
    "        filtered_alignments = []\n",
    "        for en, zh_counter in anchor_freq.items():\n",
    "            most_frequent_zh = zh_counter.most_common(1)[0][0]  # Get the most frequent `zh`\n",
    "            filtered_alignments.append(AnchorWord(en, most_frequent_zh))\n",
    "        \n",
    "        # Step 3: Sort alphabetically by `en`\n",
    "        sorted_filtered_anchors = sorted(filtered_alignments, key=lambda anchor: anchor.en)\n",
    "        \n",
    "        # Step 4: Write to file\n",
    "        with open('possible-anchors.txt', 'w') as file:\n",
    "            for alignment in sorted_filtered_anchors:\n",
    "                file.write(f\"{alignment.en} {alignment.zh}\\n\")\n",
    "    \n",
    "    def load_sorted_anchors(self, anchor_path):\n",
    "        anchors = set()\n",
    "        with open(anchor_path, 'r') as file:\n",
    "            for line in file: \n",
    "                alignment = line.strip().split(' ')\n",
    "                en = alignment[0].replace('_', ' ')\n",
    "                zh = alignment[1] \n",
    "                anchors.add((en, zh))  # Store as a tuple for paired lookup\n",
    "        self.anchor_words = anchors"
   ],
   "id": "2e89c0453ef70e40",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:56:02.961269Z",
     "start_time": "2024-11-26T23:56:02.959434Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus = ParallelCorpus()  # Initialize Corpus Object",
   "id": "eaed50fde78b37",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:16:09.845893Z",
     "start_time": "2024-11-26T23:56:04.313725Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_parallel_sentences(data_source='./FTIE/')  # Load parallel sentences from data source",
   "id": "6ae262f0a64e3b4f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:20:13.976206Z",
     "start_time": "2024-11-27T00:18:38.310314Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.generate_multi_grams()  # Generate Multi grams e.g Asian Financial Crisis -> asian_financial_crisis",
   "id": "54d953742ad51578",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:21:40.366492Z",
     "start_time": "2024-11-27T00:21:31.187121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parallel_corpus.format_parallel_sentences_for_awesome_align() # Format English Sentence With Multi Grams \n",
    "# Prepare a data source for awesome align \n",
    "\n",
    "# DATA_FILE=./zhen.src-tgt\n",
    "# MODEL_NAME_OR_PATH=./model_without_co\n",
    "# OUTPUT_FILE=./output.txt\n",
    "# OUTPUT_WORDS=./alignments.txt\n",
    "# OUTPUT_PROB=./alignments-prob.txt\n",
    "# \n",
    "# CUDA_VISIBLE_DEVICES=0 awesome-align \\\n",
    "#     --output_file=$OUTPUT_FILE \\\n",
    "#     --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "#     --data_file=$DATA_FILE \\\n",
    "#     --extraction 'softmax' \\\n",
    "#     --batch_size 32 \\\n",
    "#     --num_workers 0 \\\n",
    "#     --output_word_file=$OUTPUT_WORDS \\\n",
    "#     --output_prob_file=$OUTPUT_PROB "
   ],
   "id": "1f4cbeea37e3df17",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:36:01.735614Z",
     "start_time": "2024-11-26T09:35:56.903747Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.build_anchor_words_from_awesome_align_output('./alignments.txt')    # Generate Possible Anchor Words",
   "id": "28a996a8a95010c7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:22:29.892052Z",
     "start_time": "2024-11-27T00:22:29.886015Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_sorted_anchors('./final_anchors.txt')  # Load Final and Verified Anchors",
   "id": "b7e9f9e66c29a192",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T02:01:10.909704Z",
     "start_time": "2024-11-27T02:01:05.966184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # How would the original model translate these anchor words? \n",
    "# def translate_anchor_words(src_lang, tgt_lang, output_file):\n",
    "#     # Set the source and target languages\n",
    "#     tokenizer.src_lang = src_lang\n",
    "#     tokenizer.tgt_lang = tgt_lang\n",
    "#     forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]  # Ensure the target language is correct\n",
    "# \n",
    "#     # Translate and save results\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for index, pair in enumerate(parallel_corpus.anchor_words):\n",
    "#             # Select source and target based on direction\n",
    "#             source_anchor = pair.zh if src_lang == \"zh_CN\" else pair.en\n",
    "#             target_anchor = pair.en if src_lang == \"zh_CN\" else pair.zh\n",
    "# \n",
    "#             # Tokenize the input text\n",
    "#             inputs = tokenizer(source_anchor, return_tensors=\"pt\")\n",
    "#             # Generate translation with forced BOS token for the target language\n",
    "#             translated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "#             # Decode the translated tokens\n",
    "#             translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "# \n",
    "#             # Save the result in the text file\n",
    "#             f.write(f\"{source_anchor}; {target_anchor}; {translation.lower()}\\n\")\n",
    "# \n",
    "#             if index % 100 == 0:\n",
    "#                 print(f\"Done translating {index} / {len(parallel_corpus.anchor_words)}\")\n",
    "# \n"
   ],
   "id": "12b082515fe9f067",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T11:03:26.146342Z",
     "start_time": "2024-11-26T10:38:56.938122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Translate English to Chinese\n",
    "# translate_anchor_words(\n",
    "#     src_lang=\"en_XX\",\n",
    "#     tgt_lang=\"zh_CN\",\n",
    "#     output_file=\"unmodified_en-zh-translated_anchor_words.txt\"\n",
    "# )"
   ],
   "id": "9ffd3eea578b97ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done translating 0 / 3044\n",
      "Done translating 100 / 3044\n",
      "Done translating 200 / 3044\n",
      "Done translating 300 / 3044\n",
      "Done translating 400 / 3044\n",
      "Done translating 500 / 3044\n",
      "Done translating 600 / 3044\n",
      "Done translating 700 / 3044\n",
      "Done translating 800 / 3044\n",
      "Done translating 900 / 3044\n",
      "Done translating 1000 / 3044\n",
      "Done translating 1100 / 3044\n",
      "Done translating 1200 / 3044\n",
      "Done translating 1300 / 3044\n",
      "Done translating 1400 / 3044\n",
      "Done translating 1500 / 3044\n",
      "Done translating 1600 / 3044\n",
      "Done translating 1700 / 3044\n",
      "Done translating 1800 / 3044\n",
      "Done translating 1900 / 3044\n",
      "Done translating 2000 / 3044\n",
      "Done translating 2100 / 3044\n",
      "Done translating 2200 / 3044\n",
      "Done translating 2300 / 3044\n",
      "Done translating 2400 / 3044\n",
      "Done translating 2500 / 3044\n",
      "Done translating 2600 / 3044\n",
      "Done translating 2700 / 3044\n",
      "Done translating 2800 / 3044\n",
      "Done translating 2900 / 3044\n",
      "Done translating 3000 / 3044\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# translate_anchor_words(\n",
    "#     src_lang=\"zh_CN\",\n",
    "#     tgt_lang=\"en_XX\",\n",
    "#     output_file=\"unmodified_zh-en-translated_anchor_words.txt\"\n",
    "# )"
   ],
   "id": "833fdc7447b207a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:41:56.518971Z",
     "start_time": "2024-11-26T23:41:56.511142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_zh-en-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         zh_anchor = items[0].strip()\n",
    "#         en_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#         \n",
    "#         if translation == en_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on Chinese Anchor Words (zh->en):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ],
   "id": "b85df343acb503d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Accuracy on Chinese Anchor Words (zh->en): 0.32424441524310116\n",
      "Perfect Match Count: 987 out of 3044\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:41:53.564022Z",
     "start_time": "2024-11-26T23:41:53.558986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_en-zh-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         en_anchor = items[0].strip()\n",
    "#         zh_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#             \n",
    "#         if translation == zh_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on English Anchor Words (en->zh):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ],
   "id": "1d1c5e0d1b326585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Accuracy on English Anchor Words (en->zh): 0.32490144546649147\n",
      "Perfect Match Count: 989 out of 3044\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:35:07.931641Z",
     "start_time": "2024-11-27T01:03:02.943988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anchor_words_dict = {en: zh for en, zh in parallel_corpus.anchor_words}\n",
    "\n",
    "def refactor_sentence_with_anchors(en_sentence, chinese_sentence, anchor_words):\n",
    "    # Tokenize the sentence into words\n",
    "    words = en_sentence.split(' ')\n",
    "    modified_sentence = []\n",
    "    i = 0\n",
    "    refactored_chinese_sentence = chinese_sentence.replace(' ', '')\n",
    "    while i < len(words):\n",
    "        found = False\n",
    "        \n",
    "        # Check for multi-word anchor terms in English\n",
    "        for length in range(4, 1, -1):  # Check from 4 words (quadgram) to 2 words (bigram)\n",
    "            if i + length <= len(words):\n",
    "                multi_word_candidate = ' '.join(words[i:i+length]).lower()  # Make sure we match underscore-separated terms\n",
    "                # Iterate over the anchor words and check the English part of the pair\n",
    "                for en_term, zh_term in anchor_words:\n",
    "                    if multi_word_candidate == en_term:\n",
    "                        modified_sentence.append(f\"<{multi_word_candidate.replace(' ', '_')}>\")  # Replace with English term\n",
    "                        i += length  # Skip the words that are part of the multi-word term\n",
    "                        found = True\n",
    "                        refactored_chinese_sentence = refactored_chinese_sentence.replace(anchor_words_dict[en_term], '<'+zh_term+'>')\n",
    "                        \n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "        \n",
    "        if not found:\n",
    "            # If no multi-word term is found, just add the current word\n",
    "            modified_sentence.append(words[i])\n",
    "            i += 1\n",
    "    \n",
    "    # Return the modified sentence as a string\n",
    "    return ' '.join(modified_sentence), refactored_chinese_sentence\n",
    "\n",
    "\n",
    "# Example to refactor both English and Chinese sentences\n",
    "refactored_parallel_sentences = []\n",
    "for index, parallel_sentence in enumerate(parallel_corpus.parallel_sentences):\n",
    "    # Refactor the English sentence with anchor words\n",
    "    modified_english_sentence, modified_chinese_sentence = refactor_sentence_with_anchors(parallel_sentence.en, parallel_sentence.zh, parallel_corpus.anchor_words)\n",
    "\n",
    "    # Append the refactored sentence pair to the list\n",
    "    refactored_parallel_sentences.append(ParallelSentence(modified_english_sentence, modified_chinese_sentence))\n",
    "    if index % 1000 == 0: \n",
    "        print(\"Done refactoring\", index, \"out of\", len(parallel_corpus.parallel_sentences))"
   ],
   "id": "b3ee50f476ebc70d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done refactoring 0 out of 255860\n",
      "Done refactoring 1000 out of 255860\n",
      "Done refactoring 2000 out of 255860\n",
      "Done refactoring 3000 out of 255860\n",
      "Done refactoring 4000 out of 255860\n",
      "Done refactoring 5000 out of 255860\n",
      "Done refactoring 6000 out of 255860\n",
      "Done refactoring 7000 out of 255860\n",
      "Done refactoring 8000 out of 255860\n",
      "Done refactoring 9000 out of 255860\n",
      "Done refactoring 10000 out of 255860\n",
      "Done refactoring 11000 out of 255860\n",
      "Done refactoring 12000 out of 255860\n",
      "Done refactoring 13000 out of 255860\n",
      "Done refactoring 14000 out of 255860\n",
      "Done refactoring 15000 out of 255860\n",
      "Done refactoring 16000 out of 255860\n",
      "Done refactoring 17000 out of 255860\n",
      "Done refactoring 18000 out of 255860\n",
      "Done refactoring 19000 out of 255860\n",
      "Done refactoring 20000 out of 255860\n",
      "Done refactoring 21000 out of 255860\n",
      "Done refactoring 22000 out of 255860\n",
      "Done refactoring 23000 out of 255860\n",
      "Done refactoring 24000 out of 255860\n",
      "Done refactoring 25000 out of 255860\n",
      "Done refactoring 26000 out of 255860\n",
      "Done refactoring 27000 out of 255860\n",
      "Done refactoring 28000 out of 255860\n",
      "Done refactoring 29000 out of 255860\n",
      "Done refactoring 30000 out of 255860\n",
      "Done refactoring 31000 out of 255860\n",
      "Done refactoring 32000 out of 255860\n",
      "Done refactoring 33000 out of 255860\n",
      "Done refactoring 34000 out of 255860\n",
      "Done refactoring 35000 out of 255860\n",
      "Done refactoring 36000 out of 255860\n",
      "Done refactoring 37000 out of 255860\n",
      "Done refactoring 38000 out of 255860\n",
      "Done refactoring 39000 out of 255860\n",
      "Done refactoring 40000 out of 255860\n",
      "Done refactoring 41000 out of 255860\n",
      "Done refactoring 42000 out of 255860\n",
      "Done refactoring 43000 out of 255860\n",
      "Done refactoring 44000 out of 255860\n",
      "Done refactoring 45000 out of 255860\n",
      "Done refactoring 46000 out of 255860\n",
      "Done refactoring 47000 out of 255860\n",
      "Done refactoring 48000 out of 255860\n",
      "Done refactoring 49000 out of 255860\n",
      "Done refactoring 50000 out of 255860\n",
      "Done refactoring 51000 out of 255860\n",
      "Done refactoring 52000 out of 255860\n",
      "Done refactoring 53000 out of 255860\n",
      "Done refactoring 54000 out of 255860\n",
      "Done refactoring 55000 out of 255860\n",
      "Done refactoring 56000 out of 255860\n",
      "Done refactoring 57000 out of 255860\n",
      "Done refactoring 58000 out of 255860\n",
      "Done refactoring 59000 out of 255860\n",
      "Done refactoring 60000 out of 255860\n",
      "Done refactoring 61000 out of 255860\n",
      "Done refactoring 62000 out of 255860\n",
      "Done refactoring 63000 out of 255860\n",
      "Done refactoring 64000 out of 255860\n",
      "Done refactoring 65000 out of 255860\n",
      "Done refactoring 66000 out of 255860\n",
      "Done refactoring 67000 out of 255860\n",
      "Done refactoring 68000 out of 255860\n",
      "Done refactoring 69000 out of 255860\n",
      "Done refactoring 70000 out of 255860\n",
      "Done refactoring 71000 out of 255860\n",
      "Done refactoring 72000 out of 255860\n",
      "Done refactoring 73000 out of 255860\n",
      "Done refactoring 74000 out of 255860\n",
      "Done refactoring 75000 out of 255860\n",
      "Done refactoring 76000 out of 255860\n",
      "Done refactoring 77000 out of 255860\n",
      "Done refactoring 78000 out of 255860\n",
      "Done refactoring 79000 out of 255860\n",
      "Done refactoring 80000 out of 255860\n",
      "Done refactoring 81000 out of 255860\n",
      "Done refactoring 82000 out of 255860\n",
      "Done refactoring 83000 out of 255860\n",
      "Done refactoring 84000 out of 255860\n",
      "Done refactoring 85000 out of 255860\n",
      "Done refactoring 86000 out of 255860\n",
      "Done refactoring 87000 out of 255860\n",
      "Done refactoring 88000 out of 255860\n",
      "Done refactoring 89000 out of 255860\n",
      "Done refactoring 90000 out of 255860\n",
      "Done refactoring 91000 out of 255860\n",
      "Done refactoring 92000 out of 255860\n",
      "Done refactoring 93000 out of 255860\n",
      "Done refactoring 94000 out of 255860\n",
      "Done refactoring 95000 out of 255860\n",
      "Done refactoring 96000 out of 255860\n",
      "Done refactoring 97000 out of 255860\n",
      "Done refactoring 98000 out of 255860\n",
      "Done refactoring 99000 out of 255860\n",
      "Done refactoring 100000 out of 255860\n",
      "Done refactoring 101000 out of 255860\n",
      "Done refactoring 102000 out of 255860\n",
      "Done refactoring 103000 out of 255860\n",
      "Done refactoring 104000 out of 255860\n",
      "Done refactoring 105000 out of 255860\n",
      "Done refactoring 106000 out of 255860\n",
      "Done refactoring 107000 out of 255860\n",
      "Done refactoring 108000 out of 255860\n",
      "Done refactoring 109000 out of 255860\n",
      "Done refactoring 110000 out of 255860\n",
      "Done refactoring 111000 out of 255860\n",
      "Done refactoring 112000 out of 255860\n",
      "Done refactoring 113000 out of 255860\n",
      "Done refactoring 114000 out of 255860\n",
      "Done refactoring 115000 out of 255860\n",
      "Done refactoring 116000 out of 255860\n",
      "Done refactoring 117000 out of 255860\n",
      "Done refactoring 118000 out of 255860\n",
      "Done refactoring 119000 out of 255860\n",
      "Done refactoring 120000 out of 255860\n",
      "Done refactoring 121000 out of 255860\n",
      "Done refactoring 122000 out of 255860\n",
      "Done refactoring 123000 out of 255860\n",
      "Done refactoring 124000 out of 255860\n",
      "Done refactoring 125000 out of 255860\n",
      "Done refactoring 126000 out of 255860\n",
      "Done refactoring 127000 out of 255860\n",
      "Done refactoring 128000 out of 255860\n",
      "Done refactoring 129000 out of 255860\n",
      "Done refactoring 130000 out of 255860\n",
      "Done refactoring 131000 out of 255860\n",
      "Done refactoring 132000 out of 255860\n",
      "Done refactoring 133000 out of 255860\n",
      "Done refactoring 134000 out of 255860\n",
      "Done refactoring 135000 out of 255860\n",
      "Done refactoring 136000 out of 255860\n",
      "Done refactoring 137000 out of 255860\n",
      "Done refactoring 138000 out of 255860\n",
      "Done refactoring 139000 out of 255860\n",
      "Done refactoring 140000 out of 255860\n",
      "Done refactoring 141000 out of 255860\n",
      "Done refactoring 142000 out of 255860\n",
      "Done refactoring 143000 out of 255860\n",
      "Done refactoring 144000 out of 255860\n",
      "Done refactoring 145000 out of 255860\n",
      "Done refactoring 146000 out of 255860\n",
      "Done refactoring 147000 out of 255860\n",
      "Done refactoring 148000 out of 255860\n",
      "Done refactoring 149000 out of 255860\n",
      "Done refactoring 150000 out of 255860\n",
      "Done refactoring 151000 out of 255860\n",
      "Done refactoring 152000 out of 255860\n",
      "Done refactoring 153000 out of 255860\n",
      "Done refactoring 154000 out of 255860\n",
      "Done refactoring 155000 out of 255860\n",
      "Done refactoring 156000 out of 255860\n",
      "Done refactoring 157000 out of 255860\n",
      "Done refactoring 158000 out of 255860\n",
      "Done refactoring 159000 out of 255860\n",
      "Done refactoring 160000 out of 255860\n",
      "Done refactoring 161000 out of 255860\n",
      "Done refactoring 162000 out of 255860\n",
      "Done refactoring 163000 out of 255860\n",
      "Done refactoring 164000 out of 255860\n",
      "Done refactoring 165000 out of 255860\n",
      "Done refactoring 166000 out of 255860\n",
      "Done refactoring 167000 out of 255860\n",
      "Done refactoring 168000 out of 255860\n",
      "Done refactoring 169000 out of 255860\n",
      "Done refactoring 170000 out of 255860\n",
      "Done refactoring 171000 out of 255860\n",
      "Done refactoring 172000 out of 255860\n",
      "Done refactoring 173000 out of 255860\n",
      "Done refactoring 174000 out of 255860\n",
      "Done refactoring 175000 out of 255860\n",
      "Done refactoring 176000 out of 255860\n",
      "Done refactoring 177000 out of 255860\n",
      "Done refactoring 178000 out of 255860\n",
      "Done refactoring 179000 out of 255860\n",
      "Done refactoring 180000 out of 255860\n",
      "Done refactoring 181000 out of 255860\n",
      "Done refactoring 182000 out of 255860\n",
      "Done refactoring 183000 out of 255860\n",
      "Done refactoring 184000 out of 255860\n",
      "Done refactoring 185000 out of 255860\n",
      "Done refactoring 186000 out of 255860\n",
      "Done refactoring 187000 out of 255860\n",
      "Done refactoring 188000 out of 255860\n",
      "Done refactoring 189000 out of 255860\n",
      "Done refactoring 190000 out of 255860\n",
      "Done refactoring 191000 out of 255860\n",
      "Done refactoring 192000 out of 255860\n",
      "Done refactoring 193000 out of 255860\n",
      "Done refactoring 194000 out of 255860\n",
      "Done refactoring 195000 out of 255860\n",
      "Done refactoring 196000 out of 255860\n",
      "Done refactoring 197000 out of 255860\n",
      "Done refactoring 198000 out of 255860\n",
      "Done refactoring 199000 out of 255860\n",
      "Done refactoring 200000 out of 255860\n",
      "Done refactoring 201000 out of 255860\n",
      "Done refactoring 202000 out of 255860\n",
      "Done refactoring 203000 out of 255860\n",
      "Done refactoring 204000 out of 255860\n",
      "Done refactoring 205000 out of 255860\n",
      "Done refactoring 206000 out of 255860\n",
      "Done refactoring 207000 out of 255860\n",
      "Done refactoring 208000 out of 255860\n",
      "Done refactoring 209000 out of 255860\n",
      "Done refactoring 210000 out of 255860\n",
      "Done refactoring 211000 out of 255860\n",
      "Done refactoring 212000 out of 255860\n",
      "Done refactoring 213000 out of 255860\n",
      "Done refactoring 214000 out of 255860\n",
      "Done refactoring 215000 out of 255860\n",
      "Done refactoring 216000 out of 255860\n",
      "Done refactoring 217000 out of 255860\n",
      "Done refactoring 218000 out of 255860\n",
      "Done refactoring 219000 out of 255860\n",
      "Done refactoring 220000 out of 255860\n",
      "Done refactoring 221000 out of 255860\n",
      "Done refactoring 222000 out of 255860\n",
      "Done refactoring 223000 out of 255860\n",
      "Done refactoring 224000 out of 255860\n",
      "Done refactoring 225000 out of 255860\n",
      "Done refactoring 226000 out of 255860\n",
      "Done refactoring 227000 out of 255860\n",
      "Done refactoring 228000 out of 255860\n",
      "Done refactoring 229000 out of 255860\n",
      "Done refactoring 230000 out of 255860\n",
      "Done refactoring 231000 out of 255860\n",
      "Done refactoring 232000 out of 255860\n",
      "Done refactoring 233000 out of 255860\n",
      "Done refactoring 234000 out of 255860\n",
      "Done refactoring 235000 out of 255860\n",
      "Done refactoring 236000 out of 255860\n",
      "Done refactoring 237000 out of 255860\n",
      "Done refactoring 238000 out of 255860\n",
      "Done refactoring 239000 out of 255860\n",
      "Done refactoring 240000 out of 255860\n",
      "Done refactoring 241000 out of 255860\n",
      "Done refactoring 242000 out of 255860\n",
      "Done refactoring 243000 out of 255860\n",
      "Done refactoring 244000 out of 255860\n",
      "Done refactoring 245000 out of 255860\n",
      "Done refactoring 246000 out of 255860\n",
      "Done refactoring 247000 out of 255860\n",
      "Done refactoring 248000 out of 255860\n",
      "Done refactoring 249000 out of 255860\n",
      "Done refactoring 250000 out of 255860\n",
      "Done refactoring 251000 out of 255860\n",
      "Done refactoring 252000 out of 255860\n",
      "Done refactoring 253000 out of 255860\n",
      "Done refactoring 254000 out of 255860\n",
      "Done refactoring 255000 out of 255860\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:36:30.398503Z",
     "start_time": "2024-11-27T01:36:30.093408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('refactored_parallel_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for pair in refactored_parallel_sentences:\n",
    "        f.write(f\"{pair.en} ; {pair.zh}\\n\")\n",
    "        \n",
    "print(\"Refactored sentences saved to 'refactored_parallel_sentences.txt'\")"
   ],
   "id": "168a9282edd086b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refactored sentences saved to 'refactored_parallel_sentences.txt'\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:07.378936Z",
     "start_time": "2024-11-28T01:35:07.374507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parallel_corpus = ParallelCorpus()\n",
    "parallel_corpus.load_sorted_anchors('./final_anchors.txt')"
   ],
   "id": "2b87b2787517f1ff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:15:37.655590Z",
     "start_time": "2024-11-28T01:15:37.652918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def save_translation_to_csv(ps, output_file):\n",
    "    # Extract English and Chinese texts from the input object\n",
    "    english_text = ps.en\n",
    "    chinese_text = ps.zh\n",
    "\n",
    "    # Create a list of dictionaries containing the data to be saved in the CSV file\n",
    "    data = [{'zh': chinese_text, 'en': english_text}]\n",
    "\n",
    "    # Define the CSV file headers\n",
    "    fieldnames = ['zh', 'en']\n",
    "\n",
    "    # Check if the CSV file already exists and is non-empty\n",
    "    file_exists = os.path.isfile(output_file) and os.path.getsize(output_file) > 0\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is newly created or empty\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the data row\n",
    "        writer.writerows(data)\n"
   ],
   "id": "993fe0b6fe3f459e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:15:46.761493Z",
     "start_time": "2024-11-28T01:15:37.908794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for parallel_sentence in refactored_parallel_sentences:\n",
    "    save_translation_to_csv(parallel_sentence, 'training_data.csv')"
   ],
   "id": "67e021026b4bfc30",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:26.748617Z",
     "start_time": "2024-11-28T01:35:26.745139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens_to_be_added = []\n",
    "for (en_anchor, zh_anchor) in parallel_corpus.anchor_words:\n",
    "    tokens_to_be_added.append('<'+en_anchor.replace(' ', '_')+'>')\n",
    "    tokens_to_be_added.append('<'+zh_anchor+'>')"
   ],
   "id": "bfda039d2c0aa2fd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:28.804709Z",
     "start_time": "2024-11-28T01:35:28.329753Z"
    }
   },
   "cell_type": "code",
   "source": "from datasets import concatenate_datasets, Dataset",
   "id": "4174ece9bbe598ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vnnsnnt/Workspace/Project-NLP/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:30.044796Z",
     "start_time": "2024-11-28T01:35:30.042999Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "c09c08cb23518531",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:31.966297Z",
     "start_time": "2024-11-28T01:35:30.706015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = './training_data.csv'\n",
    "dataset1 = pd.read_csv(dataset)"
   ],
   "id": "27b3c1467e5a7aff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:32.003301Z",
     "start_time": "2024-11-28T01:35:31.996861Z"
    }
   },
   "cell_type": "code",
   "source": "dataset1",
   "id": "2bef04154912dc36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                       zh  \\\n",
       "0       大宗商品交易商来宝集团(NobleGroup)已出售其在与中粮集团(Cofco)农业<合资>...   \n",
       "1       总部位于香港的来宝集团近来忙于筹集资金，以免失去其<投资级>信用评级，这种评级对于其<核心业...   \n",
       "2       在新加坡上市的来宝集团面对<评级机构>穆迪(Moody’s)和标准普尔(Standard&P...   \n",
       "3       在哈维•温斯坦(HarveyWeinstein)的诸多成就中——电影制片人、慈善家、连环性侵...   \n",
       "4       他因性侵和三级强奸罪而获刑23年，比检方寻求的最高刑期少6年，但比其律师团队申请的宽大处理特...   \n",
       "...                                                   ...   \n",
       "255855  在距路易斯安那州海岸40英里远的海域，一支由英国石油(BP)牵头的团队在1英里深的水下创造着...   \n",
       "255856  上周，一个新的密封罩被安装在BP受损严重的墨西哥湾Macondo油井上，这是该公司接连突破深...   \n",
       "255857  当埃马纽埃尔•马克龙(EmmanuelMacron)最近在法国发起劳动力市场改革努力的时候，...   \n",
       "255858  对劳动力市场经济学家来说，10年前开始的全球经济衰退就像是一个庞大的自然实验：数十个有着不同...   \n",
       "255859  总部位于巴黎的发达国家俱乐部——经合组织(OECD)的劳动力市场经济学家安德烈亚•加内罗(A...   \n",
       "\n",
       "                                                       en  \n",
       "0       Noble Group, the commodity trader fighting all...  \n",
       "1       The Hong Kong-based company has been scramblin...  \n",
       "2       Noble, which is listed in Singapore, had been ...  \n",
       "3       To his many achievements — film producer, phil...  \n",
       "4       His 23-year jail sentence for sexual assault a...  \n",
       "...                                                   ...  \n",
       "255855  A mile down and 40 miles out to sea off the co...  \n",
       "255856  Bolting a new sealing cap to the top of the co...  \n",
       "255857  When <emmanuel_macron> embarked on his recent ...  \n",
       "255858  The <global_recession> that started a decade a...  \n",
       "255859  “Some <labour_markets> were doing great before...  \n",
       "\n",
       "[255860 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>大宗商品交易商来宝集团(NobleGroup)已出售其在与中粮集团(Cofco)农业&lt;合资&gt;...</td>\n",
       "      <td>Noble Group, the commodity trader fighting all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>总部位于香港的来宝集团近来忙于筹集资金，以免失去其&lt;投资级&gt;信用评级，这种评级对于其&lt;核心业...</td>\n",
       "      <td>The Hong Kong-based company has been scramblin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在新加坡上市的来宝集团面对&lt;评级机构&gt;穆迪(Moody’s)和标准普尔(Standard&amp;P...</td>\n",
       "      <td>Noble, which is listed in Singapore, had been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在哈维•温斯坦(HarveyWeinstein)的诸多成就中——电影制片人、慈善家、连环性侵...</td>\n",
       "      <td>To his many achievements — film producer, phil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>他因性侵和三级强奸罪而获刑23年，比检方寻求的最高刑期少6年，但比其律师团队申请的宽大处理特...</td>\n",
       "      <td>His 23-year jail sentence for sexual assault a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255855</th>\n",
       "      <td>在距路易斯安那州海岸40英里远的海域，一支由英国石油(BP)牵头的团队在1英里深的水下创造着...</td>\n",
       "      <td>A mile down and 40 miles out to sea off the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255856</th>\n",
       "      <td>上周，一个新的密封罩被安装在BP受损严重的墨西哥湾Macondo油井上，这是该公司接连突破深...</td>\n",
       "      <td>Bolting a new sealing cap to the top of the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255857</th>\n",
       "      <td>当埃马纽埃尔•马克龙(EmmanuelMacron)最近在法国发起劳动力市场改革努力的时候，...</td>\n",
       "      <td>When &lt;emmanuel_macron&gt; embarked on his recent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255858</th>\n",
       "      <td>对劳动力市场经济学家来说，10年前开始的全球经济衰退就像是一个庞大的自然实验：数十个有着不同...</td>\n",
       "      <td>The &lt;global_recession&gt; that started a decade a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255859</th>\n",
       "      <td>总部位于巴黎的发达国家俱乐部——经合组织(OECD)的劳动力市场经济学家安德烈亚•加内罗(A...</td>\n",
       "      <td>“Some &lt;labour_markets&gt; were doing great before...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255860 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:32.851998Z",
     "start_time": "2024-11-28T01:35:32.649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = dataset1[:175000] \n",
    "validation_dataset = dataset1[175000:200000] \n",
    "\n",
    "train_dataset_hf = Dataset.from_pandas(train_dataset)\n",
    "validation_dataset_hf = Dataset.from_pandas(validation_dataset)"
   ],
   "id": "88e1a268d578f51c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:35:45.709208Z",
     "start_time": "2024-11-28T01:35:36.167922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add custom tokens and resize model embeddings\n",
    "tokenizer.add_tokens(tokens_to_be_added)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "id": "d87aabc16709a055",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MBartScaledWordEmbedding(255669, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:37:03.900111Z",
     "start_time": "2024-11-28T01:35:50.283286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"zh\"], max_length=1024, padding=\"max_length\",truncation=True)\n",
    "\n",
    "    target_encodings = tokenizer(example_batch[\"en\"], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "           \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "           \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "train_dataset_tf = train_dataset_hf.map(convert_examples_to_features, batched=True, remove_columns=[\"zh\",\"en\"])\n",
    "val_dataset_tf = validation_dataset_hf.map(convert_examples_to_features, batched=True, remove_columns=[\"zh\",\"en\"])"
   ],
   "id": "179939cbc08dea45",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 175000/175000 [01:04<00:00, 2729.83 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:08<00:00, 2861.01 examples/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:37:08.378461Z",
     "start_time": "2024-11-28T01:37:08.375527Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset_tf",
   "id": "460199c4d6a45719",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 175000\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:41:08.410116Z",
     "start_time": "2024-11-28T01:41:08.407881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "id": "ff49369d74ed42b7",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:41:45.172327Z",
     "start_time": "2024-11-28T01:41:45.167780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer,TrainingArguments, Trainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='mbartTrans',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='no',\n",
    "    eval_steps=2000,\n",
    "    logging_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    fp16=False,\n",
    "    learning_rate=2e-5,\n",
    "    optim=\"adafactor\",\n",
    "    no_cuda=True  # Forces CPU training\n",
    ")\n"
   ],
   "id": "b6d8df5f5d313b02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vnnsnnt/Workspace/Project-NLP/venv/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/vnnsnnt/Workspace/Project-NLP/venv/lib/python3.9/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:41:45.999273Z",
     "start_time": "2024-11-28T01:41:45.923054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "id": "465f883b0e387f8f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:46:02.949398Z",
     "start_time": "2024-11-28T01:41:46.510223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Seq2SeqTrainer(model=model, args=training_args, tokenizer=tokenizer,\n",
    "                  data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=train_dataset_tf,\n",
    "                        eval_dataset=val_dataset_tf)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "7476111bcba9decc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c8/xv2ygj5n49l2sb8lrhn2x4yc0000gn/T/ipykernel_4558/3143773258.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(model=model, args=training_args, tokenizer=tokenizer,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='350000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    56/350000 04:02 < 436:02:54, 0.22 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Seq2SeqTrainer(model\u001B[38;5;241m=\u001B[39mmodel, args\u001B[38;5;241m=\u001B[39mtraining_args, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m      2\u001B[0m                   data_collator\u001B[38;5;241m=\u001B[39mseq2seq_data_collator,\n\u001B[1;32m      3\u001B[0m                   train_dataset\u001B[38;5;241m=\u001B[39mtrain_dataset_tf,\n\u001B[1;32m      4\u001B[0m                         eval_dataset\u001B[38;5;241m=\u001B[39mval_dataset_tf)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:2123\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2121\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2124\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:2481\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2475\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2476\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2478\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2479\u001B[0m )\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2481\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2484\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2485\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2486\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2487\u001B[0m ):\n\u001B[1;32m   2488\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2489\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:3612\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3610\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m   3611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3612\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3613\u001B[0m     \u001B[38;5;66;03m# Finally we need to normalize the loss for reporting\u001B[39;00m\n\u001B[1;32m   3614\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_items_in_batch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/accelerate/accelerator.py:2241\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2239\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2241\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9d8027c27505444"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
