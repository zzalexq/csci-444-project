{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:55:58.493445Z",
     "start_time": "2024-11-26T23:55:55.005558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import csv\n",
    "\n",
    "import stanza\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('zh') \n",
    "nlp = stanza.Pipeline('zh', processors='tokenize')\n",
    "\n",
    "# Get the set of stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(\n",
    "    {'cent', 'href=', 'http', 'says', 'told', 'year', 'ago', 'yesterday', 'since', 'last', 'past', 'next',\n",
    "     'said', 'almost', 'within', 'would', 'nearly', 'years', 'months', 'according', 'compared', 'go', 'also', \n",
    "     \"n't\"})  \n",
    "punctuation_set = set(punctuation)\n",
    "punctuation_set.update({\"’\", \"’\", '”', \"''\", \"“\", \"'s\", '--', 'b', '/b', '/strong', '–', '—'})"
   ],
   "id": "a90636181e9940f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 51.1MB/s]                    \n",
      "2024-11-26 15:55:55 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 15:55:55 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 15:55:55 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2024-11-26 15:55:56 INFO: File exists: /Users/vnnsnnt/stanza_resources/zh-hans/default.zip\n",
      "2024-11-26 15:55:58 INFO: Finished downloading models and saved to /Users/vnnsnnt/stanza_resources\n",
      "2024-11-26 15:55:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 74.8MB/s]                    \n",
      "2024-11-26 15:55:58 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 15:55:58 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 15:55:58 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsdsimp |\n",
      "=======================\n",
      "\n",
      "2024-11-26 15:55:58 INFO: Using device: cpu\n",
      "2024-11-26 15:55:58 INFO: Loading: tokenize\n",
      "/Users/vnnsnnt/Workspace/Project-NLP/venv/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-26 15:55:58 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:20:52.527556Z",
     "start_time": "2024-11-27T06:20:52.515441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Structures\n",
    "class ParallelSentence: \n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class AnchorWord:\n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class ParallelCorpus: \n",
    "    def __init__(self):\n",
    "        self.parallel_sentences = []\n",
    "        self.multi_grams_to_consider = []\n",
    "        self.anchor_words = {}\n",
    "        \n",
    "    def load_parallel_sentences(self, data_source):\n",
    "        parallel_sentences = []\n",
    "        for file in os.listdir(data_source):\n",
    "            file_path = os.path.join(data_source, file)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as data_file:\n",
    "                reader = csv.reader(data_file, delimiter=';')\n",
    "                for row in reader:\n",
    "                    if len(row) < 7: continue   # escape bad data\n",
    "                    english_content = row[5]    # get english sentences\n",
    "                    chinese_content = row[6]    # get chinese sentences\n",
    "        \n",
    "                    # break apart sentence content by @ delimiter\n",
    "                    english_sentences = english_content.split('@')  \n",
    "                    chinese_sentences = chinese_content.split('@')\n",
    "                    \n",
    "                    for english_sentence, chinese_sentence in zip(english_sentences, chinese_sentences):\n",
    "                        clean_english_sentence = english_sentence.strip()\n",
    "                        \n",
    "                        # Process the Chinese sentence with Stanza\n",
    "                        doc = nlp(chinese_sentence)  \n",
    "                        chinese_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "                        clean_chinese_sentence = \" \".join(chinese_tokens)\n",
    "                        \n",
    "                        parallel_sentences.append(ParallelSentence(clean_english_sentence, clean_chinese_sentence))\n",
    "                        \n",
    "        self.parallel_sentences = parallel_sentences\n",
    "    \n",
    "    def generate_multi_grams(self):\n",
    "        bigrams = self.extract_ngram_counts(n=2).most_common()[:5000]\n",
    "        trigrams = self.extract_ngram_counts(n=3).most_common()[:3000]\n",
    "        quadgrams = self.extract_ngram_counts(n=4).most_common()[:1000]\n",
    "        \n",
    "        multi_grams_to_consider = set()\n",
    "        # Add multi-word terms from quad_grams_to_consider\n",
    "        for quad_gram in quadgrams:\n",
    "            multi_word_term = '_'.join(quad_gram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from trigrams_to_consider\n",
    "        for trigram in trigrams:\n",
    "            multi_word_term = '_'.join(trigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from bigrams_to_consider\n",
    "        for bigram in bigrams:\n",
    "            multi_word_term = '_'.join(bigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        self.multi_grams_to_consider = multi_grams_to_consider\n",
    "        \n",
    "    @staticmethod\n",
    "    def refactor_sentence_with_multiword_term(sentence, multi_word_terms):\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split(' ')\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            found = False\n",
    "            \n",
    "            # Check for quadgrams (4-word sequences)\n",
    "            for length in range(4, 1, -1):  # Check for quadgram to bigram\n",
    "                if i + length <= len(words):\n",
    "                    multi_word_candidate = '_'.join(words[i:i+length]).lower()\n",
    "                    if multi_word_candidate in multi_word_terms:\n",
    "                        # If a match is found, replace the words with the multi-word term\n",
    "                        modified_sentence.append(multi_word_candidate)\n",
    "                        i += length\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                # If no match is found, just add the word as is\n",
    "                modified_sentence.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        # Return the modified sentence as a string\n",
    "        return ' '.join(modified_sentence)\n",
    "    \n",
    "    def extract_ngram_counts(self, n):\n",
    "        ngram_counts = Counter()\n",
    "        for parallel_sentence in self.parallel_sentences:\n",
    "            tokens = nltk.word_tokenize(parallel_sentence.en)\n",
    "            # Filter out stopwords, punctuation, and numbers\n",
    "            filtered_tokens = [token.lower() for token in tokens \n",
    "                               if token.lower() not in stop_words \n",
    "                               and token not in punctuation_set \n",
    "                               and not token.isdigit()] \n",
    "    \n",
    "            # Generate n-grams for the filtered tokens\n",
    "            ngram_list = ngrams(filtered_tokens, n)\n",
    "            # Count the frequency of each n-gram\n",
    "            ngram_counts.update(ngram_list)\n",
    "        return ngram_counts\n",
    "    \n",
    "    def format_parallel_sentences_for_awesome_align(self):\n",
    "        with open(\"zhen.src-tgt\", \"w\") as f:\n",
    "            for parallel_sentence in self.parallel_sentences:\n",
    "                modified_sentence = self.refactor_sentence_with_multiword_term(parallel_sentence.en, self.multi_grams_to_consider)\n",
    "                f.write(f\"{modified_sentence} ||| {parallel_sentence.zh}\\n\")\n",
    "    \n",
    "    def build_anchor_words_from_awesome_align_output(self, alignments_path):\n",
    "        anchor_words = []\n",
    "        with open(alignments_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                alignment_pairs = line.strip().split(' ')\n",
    "                for index, pair in enumerate(alignment_pairs):\n",
    "                    en_entry, zh_entry = pair.split('<sep>')[0], pair.split('<sep>')[1]\n",
    "                    if en_entry not in self.multi_grams_to_consider: continue\n",
    "                    # Clean the English entry\n",
    "                    cleaned_en_entry = re.sub(r'[^a-zA-Z_]', '', en_entry)\n",
    "                    \n",
    "                    # Append only if conditions are met\n",
    "                    if cleaned_en_entry:\n",
    "                        if anchor_words and anchor_words[len(anchor_words)-1].en == cleaned_en_entry:\n",
    "                            if zh_entry not in anchor_words[len(anchor_words)-1].zh:\n",
    "                                anchor_words[len(anchor_words)-1].zh += zh_entry\n",
    "                        else:\n",
    "                            anchor_words.append(AnchorWord(cleaned_en_entry, zh_entry))\n",
    "                            \n",
    "        unique_anchors = set(AnchorWord(anchor.en, anchor.zh) for anchor in anchor_words)\n",
    "        \n",
    "        # Step 1: Count frequencies of `zh` entries for each `en`\n",
    "        anchor_freq = defaultdict(Counter)\n",
    "    \n",
    "        for anchor in unique_anchors:\n",
    "            anchor_freq[anchor.en][anchor.zh] += 1\n",
    "        \n",
    "        # Step 2: Select the most frequent `zh` entry for each `en`\n",
    "        filtered_alignments = []\n",
    "        for en, zh_counter in anchor_freq.items():\n",
    "            most_frequent_zh = zh_counter.most_common(1)[0][0]  # Get the most frequent `zh`\n",
    "            filtered_alignments.append(AnchorWord(en, most_frequent_zh))\n",
    "        \n",
    "        # Step 3: Sort alphabetically by `en`\n",
    "        sorted_filtered_anchors = sorted(filtered_alignments, key=lambda anchor: anchor.en)\n",
    "        \n",
    "        # Step 4: Write to file\n",
    "        with open('possible-anchors.txt', 'w') as file:\n",
    "            for alignment in sorted_filtered_anchors:\n",
    "                file.write(f\"{alignment.en} {alignment.zh}\\n\")\n",
    "    \n",
    "    def load_sorted_anchors(self, anchor_path):\n",
    "        anchors = set()\n",
    "        with open(anchor_path, 'r') as file:\n",
    "            for line in file: \n",
    "                alignment = line.strip().split(' ')\n",
    "                en = alignment[0].replace('_', ' ')\n",
    "                zh = alignment[1] \n",
    "                anchors.add((en, zh))  # Store as a tuple for paired lookup\n",
    "        self.anchor_words = anchors"
   ],
   "id": "2e89c0453ef70e40",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:56:02.961269Z",
     "start_time": "2024-11-26T23:56:02.959434Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus = ParallelCorpus()  # Initialize Corpus Object",
   "id": "eaed50fde78b37",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:16:09.845893Z",
     "start_time": "2024-11-26T23:56:04.313725Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_parallel_sentences(data_source='./FTIE/')  # Load parallel sentences from data source",
   "id": "6ae262f0a64e3b4f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:20:13.976206Z",
     "start_time": "2024-11-27T00:18:38.310314Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.generate_multi_grams()  # Generate Multi grams e.g Asian Financial Crisis -> asian_financial_crisis",
   "id": "54d953742ad51578",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:21:40.366492Z",
     "start_time": "2024-11-27T00:21:31.187121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parallel_corpus.format_parallel_sentences_for_awesome_align() # Format English Sentence With Multi Grams \n",
    "# Prepare a data source for awesome align \n",
    "\n",
    "# DATA_FILE=./zhen.src-tgt\n",
    "# MODEL_NAME_OR_PATH=./model_without_co\n",
    "# OUTPUT_FILE=./output.txt\n",
    "# OUTPUT_WORDS=./alignments.txt\n",
    "# OUTPUT_PROB=./alignments-prob.txt\n",
    "# \n",
    "# CUDA_VISIBLE_DEVICES=0 awesome-align \\\n",
    "#     --output_file=$OUTPUT_FILE \\\n",
    "#     --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "#     --data_file=$DATA_FILE \\\n",
    "#     --extraction 'softmax' \\\n",
    "#     --batch_size 32 \\\n",
    "#     --num_workers 0 \\\n",
    "#     --output_word_file=$OUTPUT_WORDS \\\n",
    "#     --output_prob_file=$OUTPUT_PROB "
   ],
   "id": "1f4cbeea37e3df17",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:36:01.735614Z",
     "start_time": "2024-11-26T09:35:56.903747Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.build_anchor_words_from_awesome_align_output('./alignments.txt')    # Generate Possible Anchor Words",
   "id": "28a996a8a95010c7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:22:29.892052Z",
     "start_time": "2024-11-27T00:22:29.886015Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_sorted_anchors('./final_anchors.txt')  # Load Final and Verified Anchors",
   "id": "b7e9f9e66c29a192",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T02:01:10.909704Z",
     "start_time": "2024-11-27T02:01:05.966184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # How would the original model translate these anchor words? \n",
    "# def translate_anchor_words(src_lang, tgt_lang, output_file):\n",
    "#     # Set the source and target languages\n",
    "#     tokenizer.src_lang = src_lang\n",
    "#     tokenizer.tgt_lang = tgt_lang\n",
    "#     forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]  # Ensure the target language is correct\n",
    "# \n",
    "#     # Translate and save results\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for index, pair in enumerate(parallel_corpus.anchor_words):\n",
    "#             # Select source and target based on direction\n",
    "#             source_anchor = pair.zh if src_lang == \"zh_CN\" else pair.en\n",
    "#             target_anchor = pair.en if src_lang == \"zh_CN\" else pair.zh\n",
    "# \n",
    "#             # Tokenize the input text\n",
    "#             inputs = tokenizer(source_anchor, return_tensors=\"pt\")\n",
    "#             # Generate translation with forced BOS token for the target language\n",
    "#             translated_tokens = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "#             # Decode the translated tokens\n",
    "#             translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "# \n",
    "#             # Save the result in the text file\n",
    "#             f.write(f\"{source_anchor}; {target_anchor}; {translation.lower()}\\n\")\n",
    "# \n",
    "#             if index % 100 == 0:\n",
    "#                 print(f\"Done translating {index} / {len(parallel_corpus.anchor_words)}\")\n",
    "# \n"
   ],
   "id": "12b082515fe9f067",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T11:03:26.146342Z",
     "start_time": "2024-11-26T10:38:56.938122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Translate English to Chinese\n",
    "# translate_anchor_words(\n",
    "#     src_lang=\"en_XX\",\n",
    "#     tgt_lang=\"zh_CN\",\n",
    "#     output_file=\"unmodified_en-zh-translated_anchor_words.txt\"\n",
    "# )"
   ],
   "id": "9ffd3eea578b97ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done translating 0 / 3044\n",
      "Done translating 100 / 3044\n",
      "Done translating 200 / 3044\n",
      "Done translating 300 / 3044\n",
      "Done translating 400 / 3044\n",
      "Done translating 500 / 3044\n",
      "Done translating 600 / 3044\n",
      "Done translating 700 / 3044\n",
      "Done translating 800 / 3044\n",
      "Done translating 900 / 3044\n",
      "Done translating 1000 / 3044\n",
      "Done translating 1100 / 3044\n",
      "Done translating 1200 / 3044\n",
      "Done translating 1300 / 3044\n",
      "Done translating 1400 / 3044\n",
      "Done translating 1500 / 3044\n",
      "Done translating 1600 / 3044\n",
      "Done translating 1700 / 3044\n",
      "Done translating 1800 / 3044\n",
      "Done translating 1900 / 3044\n",
      "Done translating 2000 / 3044\n",
      "Done translating 2100 / 3044\n",
      "Done translating 2200 / 3044\n",
      "Done translating 2300 / 3044\n",
      "Done translating 2400 / 3044\n",
      "Done translating 2500 / 3044\n",
      "Done translating 2600 / 3044\n",
      "Done translating 2700 / 3044\n",
      "Done translating 2800 / 3044\n",
      "Done translating 2900 / 3044\n",
      "Done translating 3000 / 3044\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# translate_anchor_words(\n",
    "#     src_lang=\"zh_CN\",\n",
    "#     tgt_lang=\"en_XX\",\n",
    "#     output_file=\"unmodified_zh-en-translated_anchor_words.txt\"\n",
    "# )"
   ],
   "id": "833fdc7447b207a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:41:56.518971Z",
     "start_time": "2024-11-26T23:41:56.511142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_zh-en-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         zh_anchor = items[0].strip()\n",
    "#         en_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#         \n",
    "#         if translation == en_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on Chinese Anchor Words (zh->en):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ],
   "id": "b85df343acb503d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Accuracy on Chinese Anchor Words (zh->en): 0.32424441524310116\n",
      "Perfect Match Count: 987 out of 3044\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T23:41:53.564022Z",
     "start_time": "2024-11-26T23:41:53.558986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import Levenshtein\n",
    "# anchor_count = len(parallel_corpus.anchor_words)\n",
    "# perfect_match_count = 0\n",
    "# matching_translations = 0\n",
    "# with open(\"unmodified_en-zh-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f: \n",
    "#         items = line.split(';')\n",
    "#         en_anchor = items[0].strip()\n",
    "#         zh_anchor = items[1].strip()\n",
    "#         translation = items[2].strip()\n",
    "#             \n",
    "#         if translation == zh_anchor:\n",
    "#             perfect_match_count += 1\n",
    "#             matching_translations += 1\n",
    "# \n",
    "# print(\"Unmodified Accuracy on English Anchor Words (en->zh):\", matching_translations / anchor_count)\n",
    "# print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ],
   "id": "1d1c5e0d1b326585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Accuracy on English Anchor Words (en->zh): 0.32490144546649147\n",
      "Perfect Match Count: 989 out of 3044\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:35:07.931641Z",
     "start_time": "2024-11-27T01:03:02.943988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anchor_words_dict = {en: zh for en, zh in parallel_corpus.anchor_words}\n",
    "\n",
    "def refactor_sentence_with_anchors(en_sentence, chinese_sentence, anchor_words):\n",
    "    # Tokenize the sentence into words\n",
    "    words = en_sentence.split(' ')\n",
    "    modified_sentence = []\n",
    "    i = 0\n",
    "    refactored_chinese_sentence = chinese_sentence.replace(' ', '')\n",
    "    while i < len(words):\n",
    "        found = False\n",
    "        \n",
    "        # Check for multi-word anchor terms in English\n",
    "        for length in range(4, 1, -1):  # Check from 4 words (quadgram) to 2 words (bigram)\n",
    "            if i + length <= len(words):\n",
    "                multi_word_candidate = ' '.join(words[i:i+length]).lower()  # Make sure we match underscore-separated terms\n",
    "                # Iterate over the anchor words and check the English part of the pair\n",
    "                for en_term, zh_term in anchor_words:\n",
    "                    if multi_word_candidate == en_term:\n",
    "                        modified_sentence.append(f\"<{multi_word_candidate.replace(' ', '_')}>\")  # Replace with English term\n",
    "                        i += length  # Skip the words that are part of the multi-word term\n",
    "                        found = True\n",
    "                        refactored_chinese_sentence = refactored_chinese_sentence.replace(anchor_words_dict[en_term], '<'+zh_term+'>')\n",
    "                        \n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "        \n",
    "        if not found:\n",
    "            # If no multi-word term is found, just add the current word\n",
    "            modified_sentence.append(words[i])\n",
    "            i += 1\n",
    "    \n",
    "    # Return the modified sentence as a string\n",
    "    return ' '.join(modified_sentence), refactored_chinese_sentence\n",
    "\n",
    "\n",
    "# Example to refactor both English and Chinese sentences\n",
    "refactored_parallel_sentences = []\n",
    "for index, parallel_sentence in enumerate(parallel_corpus.parallel_sentences):\n",
    "    # Refactor the English sentence with anchor words\n",
    "    modified_english_sentence, modified_chinese_sentence = refactor_sentence_with_anchors(parallel_sentence.en, parallel_sentence.zh, parallel_corpus.anchor_words)\n",
    "\n",
    "    # Append the refactored sentence pair to the list\n",
    "    refactored_parallel_sentences.append(ParallelSentence(modified_english_sentence, modified_chinese_sentence))\n",
    "    if index % 1000 == 0: \n",
    "        print(\"Done refactoring\", index, \"out of\", len(parallel_corpus.parallel_sentences))"
   ],
   "id": "b3ee50f476ebc70d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done refactoring 0 out of 255860\n",
      "Done refactoring 1000 out of 255860\n",
      "Done refactoring 2000 out of 255860\n",
      "Done refactoring 3000 out of 255860\n",
      "Done refactoring 4000 out of 255860\n",
      "Done refactoring 5000 out of 255860\n",
      "Done refactoring 6000 out of 255860\n",
      "Done refactoring 7000 out of 255860\n",
      "Done refactoring 8000 out of 255860\n",
      "Done refactoring 9000 out of 255860\n",
      "Done refactoring 10000 out of 255860\n",
      "Done refactoring 11000 out of 255860\n",
      "Done refactoring 12000 out of 255860\n",
      "Done refactoring 13000 out of 255860\n",
      "Done refactoring 14000 out of 255860\n",
      "Done refactoring 15000 out of 255860\n",
      "Done refactoring 16000 out of 255860\n",
      "Done refactoring 17000 out of 255860\n",
      "Done refactoring 18000 out of 255860\n",
      "Done refactoring 19000 out of 255860\n",
      "Done refactoring 20000 out of 255860\n",
      "Done refactoring 21000 out of 255860\n",
      "Done refactoring 22000 out of 255860\n",
      "Done refactoring 23000 out of 255860\n",
      "Done refactoring 24000 out of 255860\n",
      "Done refactoring 25000 out of 255860\n",
      "Done refactoring 26000 out of 255860\n",
      "Done refactoring 27000 out of 255860\n",
      "Done refactoring 28000 out of 255860\n",
      "Done refactoring 29000 out of 255860\n",
      "Done refactoring 30000 out of 255860\n",
      "Done refactoring 31000 out of 255860\n",
      "Done refactoring 32000 out of 255860\n",
      "Done refactoring 33000 out of 255860\n",
      "Done refactoring 34000 out of 255860\n",
      "Done refactoring 35000 out of 255860\n",
      "Done refactoring 36000 out of 255860\n",
      "Done refactoring 37000 out of 255860\n",
      "Done refactoring 38000 out of 255860\n",
      "Done refactoring 39000 out of 255860\n",
      "Done refactoring 40000 out of 255860\n",
      "Done refactoring 41000 out of 255860\n",
      "Done refactoring 42000 out of 255860\n",
      "Done refactoring 43000 out of 255860\n",
      "Done refactoring 44000 out of 255860\n",
      "Done refactoring 45000 out of 255860\n",
      "Done refactoring 46000 out of 255860\n",
      "Done refactoring 47000 out of 255860\n",
      "Done refactoring 48000 out of 255860\n",
      "Done refactoring 49000 out of 255860\n",
      "Done refactoring 50000 out of 255860\n",
      "Done refactoring 51000 out of 255860\n",
      "Done refactoring 52000 out of 255860\n",
      "Done refactoring 53000 out of 255860\n",
      "Done refactoring 54000 out of 255860\n",
      "Done refactoring 55000 out of 255860\n",
      "Done refactoring 56000 out of 255860\n",
      "Done refactoring 57000 out of 255860\n",
      "Done refactoring 58000 out of 255860\n",
      "Done refactoring 59000 out of 255860\n",
      "Done refactoring 60000 out of 255860\n",
      "Done refactoring 61000 out of 255860\n",
      "Done refactoring 62000 out of 255860\n",
      "Done refactoring 63000 out of 255860\n",
      "Done refactoring 64000 out of 255860\n",
      "Done refactoring 65000 out of 255860\n",
      "Done refactoring 66000 out of 255860\n",
      "Done refactoring 67000 out of 255860\n",
      "Done refactoring 68000 out of 255860\n",
      "Done refactoring 69000 out of 255860\n",
      "Done refactoring 70000 out of 255860\n",
      "Done refactoring 71000 out of 255860\n",
      "Done refactoring 72000 out of 255860\n",
      "Done refactoring 73000 out of 255860\n",
      "Done refactoring 74000 out of 255860\n",
      "Done refactoring 75000 out of 255860\n",
      "Done refactoring 76000 out of 255860\n",
      "Done refactoring 77000 out of 255860\n",
      "Done refactoring 78000 out of 255860\n",
      "Done refactoring 79000 out of 255860\n",
      "Done refactoring 80000 out of 255860\n",
      "Done refactoring 81000 out of 255860\n",
      "Done refactoring 82000 out of 255860\n",
      "Done refactoring 83000 out of 255860\n",
      "Done refactoring 84000 out of 255860\n",
      "Done refactoring 85000 out of 255860\n",
      "Done refactoring 86000 out of 255860\n",
      "Done refactoring 87000 out of 255860\n",
      "Done refactoring 88000 out of 255860\n",
      "Done refactoring 89000 out of 255860\n",
      "Done refactoring 90000 out of 255860\n",
      "Done refactoring 91000 out of 255860\n",
      "Done refactoring 92000 out of 255860\n",
      "Done refactoring 93000 out of 255860\n",
      "Done refactoring 94000 out of 255860\n",
      "Done refactoring 95000 out of 255860\n",
      "Done refactoring 96000 out of 255860\n",
      "Done refactoring 97000 out of 255860\n",
      "Done refactoring 98000 out of 255860\n",
      "Done refactoring 99000 out of 255860\n",
      "Done refactoring 100000 out of 255860\n",
      "Done refactoring 101000 out of 255860\n",
      "Done refactoring 102000 out of 255860\n",
      "Done refactoring 103000 out of 255860\n",
      "Done refactoring 104000 out of 255860\n",
      "Done refactoring 105000 out of 255860\n",
      "Done refactoring 106000 out of 255860\n",
      "Done refactoring 107000 out of 255860\n",
      "Done refactoring 108000 out of 255860\n",
      "Done refactoring 109000 out of 255860\n",
      "Done refactoring 110000 out of 255860\n",
      "Done refactoring 111000 out of 255860\n",
      "Done refactoring 112000 out of 255860\n",
      "Done refactoring 113000 out of 255860\n",
      "Done refactoring 114000 out of 255860\n",
      "Done refactoring 115000 out of 255860\n",
      "Done refactoring 116000 out of 255860\n",
      "Done refactoring 117000 out of 255860\n",
      "Done refactoring 118000 out of 255860\n",
      "Done refactoring 119000 out of 255860\n",
      "Done refactoring 120000 out of 255860\n",
      "Done refactoring 121000 out of 255860\n",
      "Done refactoring 122000 out of 255860\n",
      "Done refactoring 123000 out of 255860\n",
      "Done refactoring 124000 out of 255860\n",
      "Done refactoring 125000 out of 255860\n",
      "Done refactoring 126000 out of 255860\n",
      "Done refactoring 127000 out of 255860\n",
      "Done refactoring 128000 out of 255860\n",
      "Done refactoring 129000 out of 255860\n",
      "Done refactoring 130000 out of 255860\n",
      "Done refactoring 131000 out of 255860\n",
      "Done refactoring 132000 out of 255860\n",
      "Done refactoring 133000 out of 255860\n",
      "Done refactoring 134000 out of 255860\n",
      "Done refactoring 135000 out of 255860\n",
      "Done refactoring 136000 out of 255860\n",
      "Done refactoring 137000 out of 255860\n",
      "Done refactoring 138000 out of 255860\n",
      "Done refactoring 139000 out of 255860\n",
      "Done refactoring 140000 out of 255860\n",
      "Done refactoring 141000 out of 255860\n",
      "Done refactoring 142000 out of 255860\n",
      "Done refactoring 143000 out of 255860\n",
      "Done refactoring 144000 out of 255860\n",
      "Done refactoring 145000 out of 255860\n",
      "Done refactoring 146000 out of 255860\n",
      "Done refactoring 147000 out of 255860\n",
      "Done refactoring 148000 out of 255860\n",
      "Done refactoring 149000 out of 255860\n",
      "Done refactoring 150000 out of 255860\n",
      "Done refactoring 151000 out of 255860\n",
      "Done refactoring 152000 out of 255860\n",
      "Done refactoring 153000 out of 255860\n",
      "Done refactoring 154000 out of 255860\n",
      "Done refactoring 155000 out of 255860\n",
      "Done refactoring 156000 out of 255860\n",
      "Done refactoring 157000 out of 255860\n",
      "Done refactoring 158000 out of 255860\n",
      "Done refactoring 159000 out of 255860\n",
      "Done refactoring 160000 out of 255860\n",
      "Done refactoring 161000 out of 255860\n",
      "Done refactoring 162000 out of 255860\n",
      "Done refactoring 163000 out of 255860\n",
      "Done refactoring 164000 out of 255860\n",
      "Done refactoring 165000 out of 255860\n",
      "Done refactoring 166000 out of 255860\n",
      "Done refactoring 167000 out of 255860\n",
      "Done refactoring 168000 out of 255860\n",
      "Done refactoring 169000 out of 255860\n",
      "Done refactoring 170000 out of 255860\n",
      "Done refactoring 171000 out of 255860\n",
      "Done refactoring 172000 out of 255860\n",
      "Done refactoring 173000 out of 255860\n",
      "Done refactoring 174000 out of 255860\n",
      "Done refactoring 175000 out of 255860\n",
      "Done refactoring 176000 out of 255860\n",
      "Done refactoring 177000 out of 255860\n",
      "Done refactoring 178000 out of 255860\n",
      "Done refactoring 179000 out of 255860\n",
      "Done refactoring 180000 out of 255860\n",
      "Done refactoring 181000 out of 255860\n",
      "Done refactoring 182000 out of 255860\n",
      "Done refactoring 183000 out of 255860\n",
      "Done refactoring 184000 out of 255860\n",
      "Done refactoring 185000 out of 255860\n",
      "Done refactoring 186000 out of 255860\n",
      "Done refactoring 187000 out of 255860\n",
      "Done refactoring 188000 out of 255860\n",
      "Done refactoring 189000 out of 255860\n",
      "Done refactoring 190000 out of 255860\n",
      "Done refactoring 191000 out of 255860\n",
      "Done refactoring 192000 out of 255860\n",
      "Done refactoring 193000 out of 255860\n",
      "Done refactoring 194000 out of 255860\n",
      "Done refactoring 195000 out of 255860\n",
      "Done refactoring 196000 out of 255860\n",
      "Done refactoring 197000 out of 255860\n",
      "Done refactoring 198000 out of 255860\n",
      "Done refactoring 199000 out of 255860\n",
      "Done refactoring 200000 out of 255860\n",
      "Done refactoring 201000 out of 255860\n",
      "Done refactoring 202000 out of 255860\n",
      "Done refactoring 203000 out of 255860\n",
      "Done refactoring 204000 out of 255860\n",
      "Done refactoring 205000 out of 255860\n",
      "Done refactoring 206000 out of 255860\n",
      "Done refactoring 207000 out of 255860\n",
      "Done refactoring 208000 out of 255860\n",
      "Done refactoring 209000 out of 255860\n",
      "Done refactoring 210000 out of 255860\n",
      "Done refactoring 211000 out of 255860\n",
      "Done refactoring 212000 out of 255860\n",
      "Done refactoring 213000 out of 255860\n",
      "Done refactoring 214000 out of 255860\n",
      "Done refactoring 215000 out of 255860\n",
      "Done refactoring 216000 out of 255860\n",
      "Done refactoring 217000 out of 255860\n",
      "Done refactoring 218000 out of 255860\n",
      "Done refactoring 219000 out of 255860\n",
      "Done refactoring 220000 out of 255860\n",
      "Done refactoring 221000 out of 255860\n",
      "Done refactoring 222000 out of 255860\n",
      "Done refactoring 223000 out of 255860\n",
      "Done refactoring 224000 out of 255860\n",
      "Done refactoring 225000 out of 255860\n",
      "Done refactoring 226000 out of 255860\n",
      "Done refactoring 227000 out of 255860\n",
      "Done refactoring 228000 out of 255860\n",
      "Done refactoring 229000 out of 255860\n",
      "Done refactoring 230000 out of 255860\n",
      "Done refactoring 231000 out of 255860\n",
      "Done refactoring 232000 out of 255860\n",
      "Done refactoring 233000 out of 255860\n",
      "Done refactoring 234000 out of 255860\n",
      "Done refactoring 235000 out of 255860\n",
      "Done refactoring 236000 out of 255860\n",
      "Done refactoring 237000 out of 255860\n",
      "Done refactoring 238000 out of 255860\n",
      "Done refactoring 239000 out of 255860\n",
      "Done refactoring 240000 out of 255860\n",
      "Done refactoring 241000 out of 255860\n",
      "Done refactoring 242000 out of 255860\n",
      "Done refactoring 243000 out of 255860\n",
      "Done refactoring 244000 out of 255860\n",
      "Done refactoring 245000 out of 255860\n",
      "Done refactoring 246000 out of 255860\n",
      "Done refactoring 247000 out of 255860\n",
      "Done refactoring 248000 out of 255860\n",
      "Done refactoring 249000 out of 255860\n",
      "Done refactoring 250000 out of 255860\n",
      "Done refactoring 251000 out of 255860\n",
      "Done refactoring 252000 out of 255860\n",
      "Done refactoring 253000 out of 255860\n",
      "Done refactoring 254000 out of 255860\n",
      "Done refactoring 255000 out of 255860\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T01:36:30.398503Z",
     "start_time": "2024-11-27T01:36:30.093408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('refactored_parallel_sentences.txt', 'w', encoding='utf-8') as f:\n",
    "    for pair in refactored_parallel_sentences:\n",
    "        f.write(f\"{pair.en} ; {pair.zh}\\n\")\n",
    "        \n",
    "print(\"Refactored sentences saved to 'refactored_parallel_sentences.txt'\")"
   ],
   "id": "168a9282edd086b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refactored sentences saved to 'refactored_parallel_sentences.txt'\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:31:57.072046Z",
     "start_time": "2024-11-27T06:31:57.064665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parallel_corpus = ParallelCorpus()\n",
    "parallel_corpus.load_sorted_anchors('./final_anchors.txt')"
   ],
   "id": "2b87b2787517f1ff",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:33:21.527152Z",
     "start_time": "2024-11-27T06:33:21.029208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "refactored_parallel_sentences = []\n",
    "with open('refactored_parallel_sentences.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        items = line.split(';')\n",
    "        refactored_parallel_sentences.append(ParallelSentence(items[0].strip(), items[1].strip()))\n",
    "        \n",
    "print(\"Refactored sentences loaded\")"
   ],
   "id": "18f8d66d78ef512b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refactored sentences loaded\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T06:33:44.834552Z",
     "start_time": "2024-11-27T06:33:44.831043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens_to_be_added = []\n",
    "for (en_anchor, zh_anchor) in parallel_corpus.anchor_words:\n",
    "    tokens_to_be_added.append('<'+en_anchor.replace(' ', '_')+'>')\n",
    "    tokens_to_be_added.append('<'+zh_anchor+'>')"
   ],
   "id": "bfda039d2c0aa2fd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T03:09:09.973395Z",
     "start_time": "2024-11-27T03:09:09.966712Z"
    }
   },
   "cell_type": "code",
   "source": "tokens_to_be_added",
   "id": "df83f4ac70076dbc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<business_practices>',\n",
       " '<商业行为>',\n",
       " '<deficit_countries>',\n",
       " '<赤字国家>',\n",
       " '<rising_us_interest_rates>',\n",
       " '<上升美国利率>',\n",
       " '<emerging_markets>',\n",
       " '<新兴市场>',\n",
       " '<corporate_world>',\n",
       " '<企业界>',\n",
       " '<services_firm>',\n",
       " '<服务公司>',\n",
       " '<mr_draghi>',\n",
       " '<德拉吉>',\n",
       " '<exporting_countries>',\n",
       " '<出口国>',\n",
       " '<european_parliament>',\n",
       " '<欧洲议会>',\n",
       " '<foreign_investors>',\n",
       " '<外国投资者>',\n",
       " '<investment_managers>',\n",
       " '<投资经理>',\n",
       " '<core_inflation>',\n",
       " '<核心通胀>',\n",
       " '<founder_chief_executive>',\n",
       " '<创始首席执行官>',\n",
       " '<eastern_china>',\n",
       " '<东部中国>',\n",
       " '<economic_sanctions>',\n",
       " '<经济制裁>',\n",
       " '<retail_sales>',\n",
       " '<零售>',\n",
       " '<big_business>',\n",
       " '<大企业>',\n",
       " '<challenge_us>',\n",
       " '<挑战美国>',\n",
       " '<current_crisis>',\n",
       " '<当前危机>',\n",
       " '<bilateral_trade>',\n",
       " '<双边贸易>',\n",
       " '<market_economy>',\n",
       " '<市场经济>',\n",
       " '<shares_rose>',\n",
       " '<股价上涨>',\n",
       " '<investment_products>',\n",
       " '<投资产品>',\n",
       " '<southern_guangdong>',\n",
       " '<广东>',\n",
       " '<us_sanctions>',\n",
       " '<美国制裁>',\n",
       " '<sovereign_wealth_fund>',\n",
       " '<主权财富基金>',\n",
       " '<social_media_platform>',\n",
       " '<社交媒体平台>',\n",
       " '<us_authorities>',\n",
       " '<美国当局>',\n",
       " '<emerging_market_central_banks>',\n",
       " '<新兴市场央行>',\n",
       " '<mr_blair>',\n",
       " '<布莱尔>',\n",
       " '<central_committee>',\n",
       " '<中央委员>',\n",
       " '<cut_back>',\n",
       " '<削减>',\n",
       " '<prime_minister_alexis_tsipras>',\n",
       " '<总理亚历克西斯•Tsipras>',\n",
       " '<european_business>',\n",
       " '<欧洲商>',\n",
       " '<listed_company>',\n",
       " '<上市公司>',\n",
       " '<market_forces>',\n",
       " '<市场力量>',\n",
       " '<nuclear_plant>',\n",
       " '<核电站>',\n",
       " '<hong_kong_stock_exchange>',\n",
       " '<香港交易>',\n",
       " '<emerging_world>',\n",
       " '<新兴世界>',\n",
       " '<heavy_losses>',\n",
       " '<严重亏损>',\n",
       " '<private_banking>',\n",
       " '<私人银行>',\n",
       " '<risky_assets>',\n",
       " '<风险资产>',\n",
       " '<oil_exporters>',\n",
       " '<石油出口>',\n",
       " '<overseas_investments>',\n",
       " '<海外投资>',\n",
       " '<us_president_elect_joe_biden>',\n",
       " '<美国总统当选乔•拜登(Biden)>',\n",
       " '<top_communist_party_leaders>',\n",
       " '<最高中共领导层>',\n",
       " '<transpacific_partnership_trade_deal>',\n",
       " '<跨太平洋伙伴协定>',\n",
       " '<financial_assets>',\n",
       " '<金融资产>',\n",
       " '<analysts_estimate>',\n",
       " '<分析估计>',\n",
       " '<coalition_government>',\n",
       " '<联合政府>',\n",
       " '<jackson_hole>',\n",
       " '<杰克逊霍尔>',\n",
       " '<oxford_university>',\n",
       " '<牛津University>',\n",
       " '<hong_kong_securities>',\n",
       " '<香港证券>',\n",
       " '<greek_debt>',\n",
       " '<希腊债务>',\n",
       " '<wealth_funds>',\n",
       " '<财富基金>',\n",
       " '<li_kashing>',\n",
       " '<李嘉诚>',\n",
       " '<international_fund>',\n",
       " '<国际基金>',\n",
       " '<global_financial_system>',\n",
       " '<全球金融体系>',\n",
       " '<tightly_controlled>',\n",
       " '<严格控制>',\n",
       " '<property_bubble>',\n",
       " '<房地产泡沫>',\n",
       " '<monetary_policy_committee>',\n",
       " '<货币政策委员>',\n",
       " '<accounting_standards>',\n",
       " '<会计准则>',\n",
       " '<shanghai_stock>',\n",
       " '<上海证交>',\n",
       " '<electronics_group>',\n",
       " '<电子集团>',\n",
       " '<luxury_goods>',\n",
       " '<奢侈品>',\n",
       " '<trade_disputes>',\n",
       " '<贸易争端>',\n",
       " '<equity_funds>',\n",
       " '<股票基金>',\n",
       " '<double_agent_sergei_skripal>',\n",
       " '<双谍谢尔盖•Skripal>',\n",
       " '<social_security>',\n",
       " '<社会保障>',\n",
       " '<private_investors>',\n",
       " '<私人投资>',\n",
       " '<rising_food_prices>',\n",
       " '<上涨食品价格>',\n",
       " '<san_francisco>',\n",
       " '<旧金山>',\n",
       " '<one_hong_kong>',\n",
       " '<一香港>',\n",
       " '<treasury_bonds>',\n",
       " '<国债>',\n",
       " '<fourth_largest_mobile_operator>',\n",
       " '<第四大手机运营商>',\n",
       " '<assets_management>',\n",
       " '<资产经营>',\n",
       " '<climate_change>',\n",
       " '<气候变化>',\n",
       " '<tech_groups>',\n",
       " '<科技集团>',\n",
       " '<digital_currency>',\n",
       " '<数字货币>',\n",
       " '<real_estate_market>',\n",
       " '<房地产市场>',\n",
       " '<us_commerce_department>',\n",
       " '<美国商务部>',\n",
       " '<purchasing_managers_index>',\n",
       " '<采购经理指数>',\n",
       " '<international_expansion>',\n",
       " '<国际扩张>',\n",
       " '<mortgage_lending>',\n",
       " '<抵押贷款>',\n",
       " '<singapore_airlines>',\n",
       " '<新加坡Airlines>',\n",
       " '<wall_street>',\n",
       " '<华尔街>',\n",
       " '<corporate_tax>',\n",
       " '<企业税>',\n",
       " '<deposit_rates>',\n",
       " '<存款利率>',\n",
       " '<chinese_manufacturers>',\n",
       " '<中国商>',\n",
       " '<china_foreign>',\n",
       " '<中国外交>',\n",
       " '<chinese_growth>',\n",
       " '<中国增长>',\n",
       " '<austerity_measures>',\n",
       " '<紧缩措施>',\n",
       " '<us_corporate>',\n",
       " '<美国企业>',\n",
       " '<net_asset_value>',\n",
       " '<资产净值>',\n",
       " '<management_company>',\n",
       " '<管理公司>',\n",
       " '<tax_revenues>',\n",
       " '<税收收入>',\n",
       " '<prime_minister_wen_jiabao>',\n",
       " '<总理温家宝>',\n",
       " '<singapore_state_investment_company>',\n",
       " '<新加坡国有投资公司>',\n",
       " '<president_donald>',\n",
       " '<总统唐纳德•特朗普>',\n",
       " '<financial_world>',\n",
       " '<金融界>',\n",
       " '<national_highway_traffic_safety>',\n",
       " '<国家公路交通安全>',\n",
       " '<bull_market>',\n",
       " '<牛市>',\n",
       " '<us_equities>',\n",
       " '<美国股市>',\n",
       " '<business_models>',\n",
       " '<商业模式>',\n",
       " '<china_market>',\n",
       " '<中国市场>',\n",
       " '<singapore_state_investment_agency>',\n",
       " '<新加坡政府投资机构>',\n",
       " '<mining_companies>',\n",
       " '<矿业公司>',\n",
       " '<government_spending>',\n",
       " '<政府支出>',\n",
       " '<chinese_telecoms_equipment_maker>',\n",
       " '<中国电信设备商>',\n",
       " '<china_military>',\n",
       " '<中国军力>',\n",
       " '<deputy_chief>',\n",
       " '<副首席>',\n",
       " '<currency_peg>',\n",
       " '<汇率>',\n",
       " '<energy_consumption>',\n",
       " '<能源消耗>',\n",
       " '<international_financial_centre>',\n",
       " '<国际金融中心>',\n",
       " '<data_provider>',\n",
       " '<数据商>',\n",
       " '<debt_equity>',\n",
       " '<债转>',\n",
       " '<pension_funds>',\n",
       " '<养老基金>',\n",
       " '<us_economic>',\n",
       " '<美国经济>',\n",
       " '<financial_data>',\n",
       " '<金融数据>',\n",
       " '<us_media>',\n",
       " '<美国媒体>',\n",
       " '<ageing_population>',\n",
       " '<老龄化人口>',\n",
       " '<credit_default>',\n",
       " '<信用违约>',\n",
       " '<financial_markets>',\n",
       " '<金融市场>',\n",
       " '<tax_cut>',\n",
       " '<减税>',\n",
       " '<western_markets>',\n",
       " '<西方市场>',\n",
       " '<us_economic_growth>',\n",
       " '<美国经济增长>',\n",
       " '<las_vegas_sands>',\n",
       " '<拉斯维加斯金沙>',\n",
       " '<federal_open_market>',\n",
       " '<联邦公开市场>',\n",
       " '<goldman_sachs_asset_management>',\n",
       " '<高盛金砖基金投资>',\n",
       " '<uk_financial>',\n",
       " '<英国金融>',\n",
       " '<stock_prices>',\n",
       " '<股价>',\n",
       " '<index_dropped>',\n",
       " '<指数下跌>',\n",
       " '<singapore_state_investment>',\n",
       " '<新加坡政府投资>',\n",
       " '<core_business>',\n",
       " '<核心业务>',\n",
       " '<party_officials>',\n",
       " '<党官员>',\n",
       " '<wang_jianlin>',\n",
       " '<王健林>',\n",
       " '<capital_market>',\n",
       " '<资本市场>',\n",
       " '<general_election>',\n",
       " '<大选>',\n",
       " '<national_income>',\n",
       " '<国民收入>',\n",
       " '<tiananmen_square>',\n",
       " '<天安门广场>',\n",
       " '<bottled_water>',\n",
       " '<瓶装水>',\n",
       " '<credit_default_swaps>',\n",
       " '<信用违约互换>',\n",
       " '<mr_musk>',\n",
       " '<马斯克>',\n",
       " '<global_energy>',\n",
       " '<全球能源>',\n",
       " '<capital_flows>',\n",
       " '<资本流动>',\n",
       " '<business_people>',\n",
       " '<商界人士>',\n",
       " '<consumer_market>',\n",
       " '<消费市场>',\n",
       " '<lending_rates>',\n",
       " '<贷款利率>',\n",
       " '<nuclear_ambitions>',\n",
       " '<核野心>',\n",
       " '<presidential_race>',\n",
       " '<总统竞选>',\n",
       " '<hong_kongbased_investment_firm>',\n",
       " '<香港投资公司>',\n",
       " '<current_level>',\n",
       " '<目前水平>',\n",
       " '<estate_agents>',\n",
       " '<房地产人>',\n",
       " '<london_business>',\n",
       " '<伦敦商>',\n",
       " '<microsoft_founder_bill_gates>',\n",
       " '<微软(Microsoft)创始比尔•盖茨>',\n",
       " '<us_allies>',\n",
       " '<美国盟友>',\n",
       " '<tighten_monetary_policy>',\n",
       " '<收紧货币政策>',\n",
       " '<expected_increase>',\n",
       " '<预期>',\n",
       " '<private_equity_funds>',\n",
       " '<私人股本基金>',\n",
       " '<hong_kong_stock_exchanges>',\n",
       " '<香港交易>',\n",
       " '<private_equity>',\n",
       " '<私人股本>',\n",
       " '<private_equity_firm_tpg>',\n",
       " '<私人股本公司TPG>',\n",
       " '<property_prices>',\n",
       " '<房价>',\n",
       " '<statistics_bureau>',\n",
       " '<统计局>',\n",
       " '<internet_search>',\n",
       " '<网搜索>',\n",
       " '<world_economic_forum>',\n",
       " '<世界经济论坛>',\n",
       " '<earnings_growth>',\n",
       " '<盈利增长>',\n",
       " '<rating_agencies>',\n",
       " '<评级机构>',\n",
       " '<debt_obligations>',\n",
       " '<债务>',\n",
       " '<economic_news>',\n",
       " '<经济消息>',\n",
       " '<chief_executives>',\n",
       " '<首席执行官>',\n",
       " '<competitive_advantage>',\n",
       " '<竞争优势>',\n",
       " '<assets_including>',\n",
       " '<资产包括>',\n",
       " '<mr_macron>',\n",
       " '<马克龙>',\n",
       " '<real_gross_domestic>',\n",
       " '<实际总值国内>',\n",
       " '<japanese_economy>',\n",
       " '<日本经济>',\n",
       " '<oil_exports>',\n",
       " '<石油出口>',\n",
       " '<global_companies>',\n",
       " '<全球企业>',\n",
       " '<asian_stock_markets>',\n",
       " '<亚洲股市>',\n",
       " '<stock_markets>',\n",
       " '<股市>',\n",
       " '<us_businesses>',\n",
       " '<美国企业>',\n",
       " '<market_regulator>',\n",
       " '<市场监管机构>',\n",
       " '<shadow_banking_system>',\n",
       " '<影子银行体系>',\n",
       " '<royal_bank>',\n",
       " '<皇家银行>',\n",
       " '<us_trade>',\n",
       " '<美国贸易>',\n",
       " '<criminal_charges>',\n",
       " '<刑事指控>',\n",
       " '<wealth_management>',\n",
       " '<财富管理>',\n",
       " '<crude_oil_prices>',\n",
       " '<原油价格>',\n",
       " '<hong_kong_chief_executive>',\n",
       " '<香港特首>',\n",
       " '<oil_producers>',\n",
       " '<石油生产>',\n",
       " '<president_vladimir_putin>',\n",
       " '<总统弗拉基米尔•普京>',\n",
       " '<income_inequality>',\n",
       " '<收入不平等>',\n",
       " '<shanghai_composite_index>',\n",
       " '<上证综合指数>',\n",
       " '<state_enterprises>',\n",
       " '<国有企业>',\n",
       " '<global_investment>',\n",
       " '<全球投资>',\n",
       " '<korean_war>',\n",
       " '<朝鲜战争>',\n",
       " '<hedge_fund_managers>',\n",
       " '<对冲基金经理>',\n",
       " '<kim_jong_un_regime>',\n",
       " '<金正恩政权>',\n",
       " '<first_world_war>',\n",
       " '<第一世界大战>',\n",
       " '<british_colony>',\n",
       " '<英国殖民>',\n",
       " '<vladimir_putin>',\n",
       " '<弗拉基米尔•普京>',\n",
       " '<missile_defence>',\n",
       " '<导弹防御>',\n",
       " '<chinese_bank>',\n",
       " '<中国银行>',\n",
       " '<mr_tsang>',\n",
       " '<曾荫权>',\n",
       " '<president_emmanuel>',\n",
       " '<总统•埃马纽埃尔>',\n",
       " '<renminbi_exchange_rate>',\n",
       " '<人民汇率>',\n",
       " '<us_initial_public_offering>',\n",
       " '<美国首次(公开发行)>',\n",
       " '<fiscal_deficit>',\n",
       " '<财政赤字>',\n",
       " '<benchmark_index>',\n",
       " '<基准指数>',\n",
       " '<high_oil_prices>',\n",
       " '<高油价>',\n",
       " '<search_engine>',\n",
       " '<搜索引擎>',\n",
       " '<us_supreme_court>',\n",
       " '<美国最高法院>',\n",
       " '<domestic_demand>',\n",
       " '<国内需求>',\n",
       " '<us_navy>',\n",
       " '<美国海军>',\n",
       " '<south_america>',\n",
       " '<南美>',\n",
       " '<foreign_investments>',\n",
       " '<外国投资>',\n",
       " '<global_output>',\n",
       " '<全球产出>',\n",
       " '<cash_flow>',\n",
       " '<现金流>',\n",
       " '<us_energy_information_administration>',\n",
       " '<美国能源情报署>',\n",
       " '<homeland_security>',\n",
       " '<国土安全>',\n",
       " '<coking_coal>',\n",
       " '<炼焦煤>',\n",
       " '<western_world>',\n",
       " '<西方世界>',\n",
       " '<working_hours>',\n",
       " '<工作时间>',\n",
       " '<living_standards>',\n",
       " '<生活水平>',\n",
       " '<stimulus_packages>',\n",
       " '<刺激方案>',\n",
       " '<david_cameron>',\n",
       " '<戴维•卡梅伦>',\n",
       " '<dotcom_bubble>',\n",
       " '<互联网泡沫>',\n",
       " '<air_pollution>',\n",
       " '<空气污染>',\n",
       " '<solar_panels>',\n",
       " '<太阳电池>',\n",
       " '<national_petroleum>',\n",
       " '<石油气>',\n",
       " '<sovereign_wealth>',\n",
       " '<主权财富>',\n",
       " '<trading_partners>',\n",
       " '<贸易伙伴>',\n",
       " '<controlling_shareholder>',\n",
       " '<控股股东>',\n",
       " '<foreign_minister_wang_yi>',\n",
       " '<外长王毅>',\n",
       " '<interest_rate_policy>',\n",
       " '<利率政策>',\n",
       " '<interest_rate_cuts>',\n",
       " '<降息>',\n",
       " '<energy_use>',\n",
       " '<能源使用>',\n",
       " '<board_meeting>',\n",
       " '<董事会议>',\n",
       " '<party_secretary>',\n",
       " '<书记>',\n",
       " '<property_rights>',\n",
       " '<产权>',\n",
       " '<fixed_exchange_rate_system>',\n",
       " '<固定汇率体系>',\n",
       " '<capital_raising>',\n",
       " '<融资>',\n",
       " '<public_policy>',\n",
       " '<公共政策>',\n",
       " '<urgent_need>',\n",
       " '<迫切需要>',\n",
       " '<mr_st_pierre_sr>',\n",
       " '<圣皮尔>',\n",
       " '<drug_administration>',\n",
       " '<药品局>',\n",
       " '<solar_panel>',\n",
       " '<太阳电池>',\n",
       " '<southeast_asian_nations>',\n",
       " '<东南亚国家>',\n",
       " '<bloomberg_barclays_global_aggregate>',\n",
       " '<彭博巴克莱Global综合>',\n",
       " '<foreign_exchange_reserves>',\n",
       " '<外汇储备>',\n",
       " '<export_growth>',\n",
       " '<出口增长>',\n",
       " '<tim_geithner>',\n",
       " '<Tim蒂姆•盖特纳(Geithner)>',\n",
       " '<chinese_equities>',\n",
       " '<中国股市>',\n",
       " '<asia_pacific>',\n",
       " '<亚太>',\n",
       " '<survey_found>',\n",
       " '<调查发现>',\n",
       " '<eu_member_states>',\n",
       " '<欧盟成员国>',\n",
       " '<asian_financial_crisis>',\n",
       " '<亚洲金融危机>',\n",
       " '<economic_cycle>',\n",
       " '<经济周期>',\n",
       " '<us_private_equity_firm>',\n",
       " '<美国私人股本公司>',\n",
       " '<asiapacific_economic_cooperation_summit>',\n",
       " '<亚太经合组)峰会>',\n",
       " '<prime_minister_narendra>',\n",
       " '<总理纳伦德拉•Narendra>',\n",
       " '<china_first>',\n",
       " '<中国首次>',\n",
       " '<mobile_phones>',\n",
       " '<手机>',\n",
       " '<china_development>',\n",
       " '<中国开发>',\n",
       " '<supply_chains>',\n",
       " '<供应链>',\n",
       " '<book_value>',\n",
       " '<账面价值>',\n",
       " '<coronavirus_cases>',\n",
       " '<冠状病毒病例>',\n",
       " '<united_arab>',\n",
       " '<阿联酋>',\n",
       " '<prince_mohammed_bin_salman>',\n",
       " '<王储穆罕默德•本bin萨勒曼>',\n",
       " '<renewable_energy>',\n",
       " '<再生能源>',\n",
       " '<eurozone_sovereign_debt_crisis>',\n",
       " '<欧区主权债务危机>',\n",
       " '<inflation_target>',\n",
       " '<通胀目标>',\n",
       " '<insurance_companies>',\n",
       " '<保险公司>',\n",
       " '<us_president_donald>',\n",
       " '<美国总统唐纳德•特朗普>',\n",
       " '<financial_conditions>',\n",
       " '<金融环境>',\n",
       " '<north_korea_leader_kim>',\n",
       " '<朝鲜人恩>',\n",
       " '<market_sentiment>',\n",
       " '<市场情绪>',\n",
       " '<mike_pompeo>',\n",
       " '<迈克•Pompeo>',\n",
       " '<internet_companies>',\n",
       " '<网公司>',\n",
       " '<domestic_market>',\n",
       " '<国内市场>',\n",
       " '<rising_prices>',\n",
       " '<上涨价格>',\n",
       " '<rights_groups>',\n",
       " '<人权组织>',\n",
       " '<better_known>',\n",
       " '<更熟知>',\n",
       " '<iron_ore>',\n",
       " '<铁矿石>',\n",
       " '<energy_policy>',\n",
       " '<能源政策>',\n",
       " '<personal_computer>',\n",
       " '<个人电脑>',\n",
       " '<us_china_trade_war>',\n",
       " '<美中贸易战>',\n",
       " '<bank_deposits>',\n",
       " '<银行存款>',\n",
       " '<presidential_campaign>',\n",
       " '<总统竞选>',\n",
       " '<us_regulators>',\n",
       " '<美国监管机构>',\n",
       " '<world_stage>',\n",
       " '<世界舞台>',\n",
       " '<ratesetting_federal_open_market>',\n",
       " '<利率制定联邦公开市场>',\n",
       " '<iron_ore_price_negotiations>',\n",
       " '<铁矿石价格谈判>',\n",
       " '<us_tariffs>',\n",
       " '<美国关税>',\n",
       " '<us_equity>',\n",
       " '<美国股市>',\n",
       " '<us_trade_deficit>',\n",
       " '<美国贸易逆差>',\n",
       " '<north_american>',\n",
       " '<北美>',\n",
       " '<prince_alwaleed_bin_talal>',\n",
       " '<PrinceAlwaleed•bin•塔拉尔Talal>',\n",
       " '<debt_restructuring>',\n",
       " '<债务重组>',\n",
       " '<edward_snowden>',\n",
       " '<爱德华•斯诺登>',\n",
       " '<prices_rose>',\n",
       " '<价格上涨>',\n",
       " '<us_dollar>',\n",
       " '<美元>',\n",
       " '<global_private_equity_firms>',\n",
       " '<全球私人股本公司>',\n",
       " '<rich_nations>',\n",
       " '<富裕国家>',\n",
       " '<narendra_modi>',\n",
       " '<纳伦德拉•莫迪>',\n",
       " '<hedge_funds>',\n",
       " '<对冲基金>',\n",
       " '<chinese_securities>',\n",
       " '<中国证券>',\n",
       " '<local_government_financing_vehicles>',\n",
       " '<地方政府融资平台>',\n",
       " '<mining_industry>',\n",
       " '<矿业>',\n",
       " '<deal_agreed>',\n",
       " '<协议达成>',\n",
       " '<massachusetts_institute>',\n",
       " '<麻省学院>',\n",
       " '<cut_emissions>',\n",
       " '<减排>',\n",
       " '<global_manufacturing>',\n",
       " '<全球制造>',\n",
       " '<mr_bernanke>',\n",
       " '<伯南克>',\n",
       " '<commodity_futures_trading_commission>',\n",
       " '<商品期货交易委员会>',\n",
       " '<mr_romney>',\n",
       " '<罗姆尼>',\n",
       " '<china_economic_slowdown>',\n",
       " '<中国经济放缓>',\n",
       " '<infrastructure_spending>',\n",
       " '<设施支出>',\n",
       " '<net_income>',\n",
       " '<净利润>',\n",
       " '<food_price>',\n",
       " '<食品价格>',\n",
       " '<fourth_quarter>',\n",
       " '<第四季度>',\n",
       " '<international_law>',\n",
       " '<国际法>',\n",
       " '<chinese_military>',\n",
       " '<中国军方>',\n",
       " '<interest_rate_rise>',\n",
       " '<加息>',\n",
       " '<reach_agreement>',\n",
       " '<达成协议>',\n",
       " '<taiwan_government>',\n",
       " '<台湾政府>',\n",
       " '<futures_trading_commission>',\n",
       " '<交易委员会>',\n",
       " '<prices_rise>',\n",
       " '<价格上涨>',\n",
       " '<us_officials>',\n",
       " '<美国官员>',\n",
       " '<renminbi_qualified_foreign_institutional>',\n",
       " '<人民合格境外机构>',\n",
       " '<trade_negotiations>',\n",
       " '<贸易谈判>',\n",
       " '<investment_officer>',\n",
       " '<投资官>',\n",
       " '<direct_investment>',\n",
       " '<直接投资>',\n",
       " '<stock_market_bubble_burst>',\n",
       " '<股市泡沫破裂>',\n",
       " '<armed_forces>',\n",
       " '<武装部队>',\n",
       " '<keep_interest_rates_low>',\n",
       " '<保持利率低>',\n",
       " '<financial_stability_board>',\n",
       " '<金融稳定会>',\n",
       " '<commerce_secretary_wilbur_ross>',\n",
       " '<商务部长威尔伯•罗斯>',\n",
       " '<state_oil_company>',\n",
       " '<国有石油公司>',\n",
       " '<us_technology>',\n",
       " '<美国科技>',\n",
       " '<annual_revenues>',\n",
       " '<年度收入>',\n",
       " '<japanese_consumer_electronics_group>',\n",
       " '<日本消费电子集团>',\n",
       " '<data_protection>',\n",
       " '<数据保护>',\n",
       " '<time_beijing>',\n",
       " '<时间北京>',\n",
       " '<price_increases>',\n",
       " '<价格上涨>',\n",
       " '<big_impact>',\n",
       " '<巨大影响>',\n",
       " '<social_security_fund>',\n",
       " '<社会保障基金>',\n",
       " '<bank_balance>',\n",
       " '<银行债表>',\n",
       " '<fox_news>',\n",
       " '<福克斯新闻>',\n",
       " '<apple_chief_executive>',\n",
       " '<苹果首席执行官>',\n",
       " '<central_business_district>',\n",
       " '<中央商务区>',\n",
       " '<emerging_market_equities>',\n",
       " '<新兴市场股市>',\n",
       " '<republican_party>',\n",
       " '<共和党>',\n",
       " '<free_cash_flow>',\n",
       " '<自由现金流>',\n",
       " '<new_york_stock>',\n",
       " '<纽约证交>',\n",
       " '<flagship_emerging_markets_index>',\n",
       " '<旗舰新兴市场指数>',\n",
       " '<us_rival>',\n",
       " '<美国对手>',\n",
       " '<us_commerce>',\n",
       " '<美国商务>',\n",
       " '<government_bond_yields>',\n",
       " '<政府债券收益率>',\n",
       " '<open_letter>',\n",
       " '<公开信>',\n",
       " '<cut_us_interest_rates>',\n",
       " '<减息美联>',\n",
       " '<global_credit>',\n",
       " '<全球信贷>',\n",
       " '<nuclear_power_station>',\n",
       " '<核电站>',\n",
       " '<chief_china>',\n",
       " '<首席中国>',\n",
       " '<ant_financial>',\n",
       " '<AntFinancial>',\n",
       " '<currency_war>',\n",
       " '<汇率战争>',\n",
       " '<paris_climate_change_accord>',\n",
       " '<巴黎气候变化协定>',\n",
       " '<investment_opportunities>',\n",
       " '<投资机会>',\n",
       " '<democratic_party>',\n",
       " '<民主党>',\n",
       " '<china_national_offshore_oil>',\n",
       " '<中海油(>',\n",
       " '<computer_science>',\n",
       " '<计算机科学>',\n",
       " '<bull_run>',\n",
       " '<牛市>',\n",
       " '<high_speed_rail>',\n",
       " '<高铁>',\n",
       " '<us_national_security>',\n",
       " '<美国国家安全>',\n",
       " '<potential_growth>',\n",
       " '<潜在增长>',\n",
       " '<western_banks>',\n",
       " '<西方银行>',\n",
       " '<sri_lanka>',\n",
       " '<斯里兰卡>',\n",
       " '<japanese_yen>',\n",
       " '<日元>',\n",
       " '<record_lows>',\n",
       " '<纪录低点>',\n",
       " '<international_trade>',\n",
       " '<国际贸易>',\n",
       " '<productivity_growth>',\n",
       " '<生产率增长>',\n",
       " '<us_private_equity_groups>',\n",
       " '<美国私人股本集团>',\n",
       " '<air_force>',\n",
       " '<空军>',\n",
       " '<technology_stocks>',\n",
       " '<科技股>',\n",
       " '<wealth_managers>',\n",
       " '<财富管理>',\n",
       " '<current_account_deficits>',\n",
       " '<经常账户赤字>',\n",
       " '<local_currency>',\n",
       " '<本币>',\n",
       " '<official_statistics>',\n",
       " '<官方数据>',\n",
       " '<south_korean_president>',\n",
       " '<韩国总统>',\n",
       " '<antidumping_duties>',\n",
       " '<反倾销税>',\n",
       " '<retirement_age>',\n",
       " '<退休年龄>',\n",
       " '<graduate_management_admission_council>',\n",
       " '<研究生管理入学考试委员会>',\n",
       " '<precious_metal>',\n",
       " '<贵金属>',\n",
       " '<boost_growth>',\n",
       " '<促进增长>',\n",
       " '<foreign_brands>',\n",
       " '<外国品牌>',\n",
       " '<us_internet>',\n",
       " '<美国网>',\n",
       " '<private_equity_firm>',\n",
       " '<私人股本公司>',\n",
       " '<working_conditions>',\n",
       " '<工作条件>',\n",
       " '<value_chain>',\n",
       " '<价值链>',\n",
       " '<net_profit>',\n",
       " '<净利润>',\n",
       " '<wells_fargo>',\n",
       " '<富国Fargo>',\n",
       " '<operating_profit>',\n",
       " '<营业利润>',\n",
       " '<law_firm>',\n",
       " '<律师所>',\n",
       " '<stocks_fell>',\n",
       " '<股市下跌>',\n",
       " '<international_banks>',\n",
       " '<国际银行>',\n",
       " '<social_networks>',\n",
       " '<社交网络>',\n",
       " '<convertible_bonds>',\n",
       " '<转换债券>',\n",
       " '<economic_success>',\n",
       " '<经济成功>',\n",
       " '<mr_cameron>',\n",
       " '<卡梅伦>',\n",
       " '<open_market_committee>',\n",
       " '<公开市场委员>',\n",
       " '<federal_deposit_insurance_corporation>',\n",
       " '<联邦存款保险公司>',\n",
       " '<prime_minister_malcolm_turnbull>',\n",
       " '<总理马尔科姆•特恩Turnbull>',\n",
       " '<dow_jones_industrial_average>',\n",
       " '<道琼斯Industrial平均>',\n",
       " '<chinese_officials>',\n",
       " '<中国官员>',\n",
       " '<inner_mongolia>',\n",
       " '<蒙古>',\n",
       " '<record_low>',\n",
       " '<纪录低>',\n",
       " '<state_investment_agency>',\n",
       " '<国家投资机构>',\n",
       " '<economy_slows>',\n",
       " '<经济放缓>',\n",
       " '<saudi_aramco>',\n",
       " '<沙特阿美>',\n",
       " '<global_financial_centre>',\n",
       " '<全球金融中心>',\n",
       " '<jamie_dimon>',\n",
       " '<杰米•Dimon>',\n",
       " '<defence_ministry>',\n",
       " '<国防部>',\n",
       " '<apple_chief_executive_tim>',\n",
       " '<苹果首席执行官蒂姆·库克>',\n",
       " '<singapore_sovereign_wealth_fund>',\n",
       " '<新加坡主权财富基金>',\n",
       " '<state_power>',\n",
       " '<国力>',\n",
       " '<global_economic_downturn>',\n",
       " '<全球经济低迷>',\n",
       " '<current_account_surpluses>',\n",
       " '<经常账户盈余>',\n",
       " '<us_central_intelligence_agency>',\n",
       " '<美国中央情报局>',\n",
       " '<prices_rising>',\n",
       " '<油价涨至>',\n",
       " '<investment_trust>',\n",
       " '<投资信托>',\n",
       " '<bubble_burst>',\n",
       " '<泡沫破裂>',\n",
       " '<mutual_funds>',\n",
       " '<共同基金>',\n",
       " '<london_metal_exchange>',\n",
       " '<伦敦金属交易>',\n",
       " '<retail_investors>',\n",
       " '<散户投资>',\n",
       " '<taiwan_strait>',\n",
       " '<台湾海峡>',\n",
       " '<british_prime_minister>',\n",
       " '<英国首相>',\n",
       " '<chinese_real_estate_market>',\n",
       " '<中国房地产市场>',\n",
       " '<developed_economies>',\n",
       " '<发达经济体>',\n",
       " '<world_health_organisation>',\n",
       " '<世界卫生组织>',\n",
       " '<asia_infrastructure_investment_bank>',\n",
       " '<亚洲设施投资银行>',\n",
       " '<damage_done>',\n",
       " '<损害造成>',\n",
       " '<financial_statements>',\n",
       " '<财务报表>',\n",
       " '<communist_party_central_committee>',\n",
       " '<中共三中全会>',\n",
       " '<gas_reserves>',\n",
       " '<气储量>',\n",
       " '<new_york_city_mayor>',\n",
       " '<纽约比尔•德市市长>',\n",
       " '<wall_street_bank>',\n",
       " '<华尔街银行>',\n",
       " '<fastest_rate>',\n",
       " '<最快速度>',\n",
       " '<auto_industry>',\n",
       " '<汽车业>',\n",
       " '<conference_call>',\n",
       " '<会议>',\n",
       " '<us_congress>',\n",
       " '<美国国会>',\n",
       " '<east_asia>',\n",
       " '<东亚>',\n",
       " '<party_chief>',\n",
       " '<市委书记>',\n",
       " '<cyber_security>',\n",
       " '<网络安全>',\n",
       " '<supreme_leader>',\n",
       " '<最高领导人>',\n",
       " '<london_business_school>',\n",
       " '<伦敦商学院>',\n",
       " '<economic_slowdown>',\n",
       " '<经济放缓>',\n",
       " '<energy_group>',\n",
       " '<能源集团>',\n",
       " '<mr_zhou>',\n",
       " '<周小川>',\n",
       " '<export_markets>',\n",
       " '<出口市场>',\n",
       " '<troubled_asset_relief_programme>',\n",
       " '<问题资产救助计划>',\n",
       " '<earnings_per_share_growth>',\n",
       " '<每股增长>',\n",
       " '<capital_inflows>',\n",
       " '<资本流入>',\n",
       " '<profit_margins>',\n",
       " '<利润率>',\n",
       " '<shadow_banking>',\n",
       " '<影子银行>',\n",
       " '<capital_expenditure>',\n",
       " '<资本支出>',\n",
       " '<ore_prices>',\n",
       " '<矿石价格>',\n",
       " '<middle_classes>',\n",
       " '<中产阶级>',\n",
       " '<account_deficit>',\n",
       " '<账户差距>',\n",
       " '<shenzhen_special_economic_zone>',\n",
       " '<深圳特区经济>',\n",
       " '<us_security>',\n",
       " '<美国安全>',\n",
       " '<international_capital>',\n",
       " '<国际资本>',\n",
       " '<new_york_mercantile_exchange>',\n",
       " '<纽约商品交易>',\n",
       " '<slowing_economic>',\n",
       " '<放缓经济>',\n",
       " '<financial_services_companies>',\n",
       " '<金融服务公司>',\n",
       " '<housing_market>',\n",
       " '<房地产市场>',\n",
       " '<japanese_companies>',\n",
       " '<日本企业>',\n",
       " '<chinese_communist>',\n",
       " '<中国共产>',\n",
       " '<philippine_president_rodrigo_duterte>',\n",
       " '<菲律宾总统罗德里戈•杜特尔特(Duterte)>',\n",
       " '<services_company>',\n",
       " '<服务公司>',\n",
       " '<face_value>',\n",
       " '<面值>',\n",
       " '<bad_debts>',\n",
       " '<坏账>',\n",
       " '<doubledigit_growth>',\n",
       " '<两位增长>',\n",
       " '<chinese_authorities>',\n",
       " '<中国当局>',\n",
       " '<south_east>',\n",
       " '<东南>',\n",
       " '<tax_authorities>',\n",
       " '<税务机关>',\n",
       " '<mr_bush>',\n",
       " '<布什>',\n",
       " '<banks_lend>',\n",
       " '<银行拆借>',\n",
       " '<uk_government>',\n",
       " '<英国政府>',\n",
       " '<chinese_communist_party>',\n",
       " '<中国共产党>',\n",
       " '<eu_leaders>',\n",
       " '<欧盟领导人>',\n",
       " '<great_wall>',\n",
       " '<长城>',\n",
       " '<rising_demand>',\n",
       " '<上升需求>',\n",
       " '<services_sector>',\n",
       " '<服务业>',\n",
       " '<steven_mnuchin>',\n",
       " '<史蒂芬•姆钦Mnuchin>',\n",
       " '<return_equity>',\n",
       " '<回报股票>',\n",
       " '<gold_standard>',\n",
       " '<黄金标准>',\n",
       " '<money_market_funds>',\n",
       " '<货币市场基金>',\n",
       " '<emerging_market_economies>',\n",
       " '<新兴市场经济体>',\n",
       " '<big_tech>',\n",
       " '<大型科技>',\n",
       " '<asiapacific_economic_cooperation_forum>',\n",
       " '<亚太经合组论坛>',\n",
       " ...]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T03:43:22.818200Z",
     "start_time": "2024-11-27T03:43:15.281964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add custom tokens and resize model embeddings\n",
    "tokenizer.add_tokens(tokens_to_be_added)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Split sentences for training, validation, and test sets\n",
    "train_sentences = refactored_parallel_sentences[0:10000]\n",
    "validation_sentences = refactored_parallel_sentences[10000:13000]\n",
    "test_sentences = refactored_parallel_sentences[13000:15000]\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class ParallelDataset(Dataset):\n",
    "    def __init__(self, inputs, attention_mask, targets):\n",
    "        self.inputs = inputs\n",
    "        self.attention_mask = attention_mask\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure that we're getting a single example from the lists\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(parallel_sentences, src_lang=\"en_XX\", tgt_lang=\"zh_CN\"):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "\n",
    "    inputs = []\n",
    "    attention_mask = []\n",
    "    targets = []\n",
    "    \n",
    "    for sentence in parallel_sentences:\n",
    "        input_text = sentence.en  # English sentence\n",
    "        target_text = sentence.zh # Chinese sentence\n",
    "\n",
    "        # Tokenize the source (English) and target (Chinese) sentences\n",
    "        input_tokens = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n",
    "        target_tokens = tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=1024, return_tensors=\"pt\")\n",
    "        \n",
    "        # Convert tensors to lists of integers\n",
    "        inputs.append(input_tokens['input_ids'].squeeze(0).tolist())  # Remove batch dimension\n",
    "        attention_mask.append(input_tokens['attention_mask'].squeeze(0).tolist())  # Remove batch dimension\n",
    "        targets.append(target_tokens['input_ids'].squeeze(0).tolist())  # Remove batch dimension\n",
    "\n",
    "    # Create a Dataset object for PyTorch DataLoader\n",
    "    dataset = ParallelDataset(inputs, attention_mask, targets)\n",
    "\n",
    "    return dataset"
   ],
   "id": "240a71bf8b67d7bd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T03:43:24.567488Z",
     "start_time": "2024-11-27T03:43:23.875071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess and create datasets\n",
    "train_dataset = preprocess_data(train_sentences)\n",
    "# validation_dataset = preprocess_data(validation_sentences)\n",
    "# test_dataset = preprocess_data(test_sentences)\n",
    "\n",
    "\n",
    "\n",
    "# # Create DataLoader objects\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# validation_dataloader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ],
   "id": "38ce711124752527",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T03:43:48.206445Z",
     "start_time": "2024-11-27T03:43:48.116175Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2b449cc635ab5800",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ParallelDataset' object has no attribute '_info'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[92], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:709\u001B[0m, in \u001B[0;36mDataset.features\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeatures\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Features:\n\u001B[0;32m--> 709\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\n\u001B[1;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# this is already checked in __init__\u001B[39;00m\n\u001B[1;32m    711\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeatures can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be None in a Dataset object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:205\u001B[0m, in \u001B[0;36mDatasetInfoMixin.features\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeatures\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Features]:\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_info\u001B[49m\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ParallelDataset' object has no attribute '_info'"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T03:40:56.703752Z",
     "start_time": "2024-11-27T03:40:56.570193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Move input tensors to GPU/CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Don't forget the attention mask\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Zero the gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, \n",
    "                         attention_mask=attention_mask, \n",
    "                         labels=labels)  # Make sure to pass attention mask\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Step optimizer and scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update the loss for this batch\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Evaluate on the validation set after each epoch\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    validation_loss = 0\n",
    "    with torch.no_grad():  # No gradient computation for validation\n",
    "        for batch in validation_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass on the validation set\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {validation_loss / len(validation_dataloader)}\")\n",
    "\n",
    "# Save the model and tokenizer after training\n",
    "model.save_pretrained('./model')\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ],
   "id": "7f5901cde4f8b12c",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[87], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Set the model to training mode\u001B[39;00m\n\u001B[1;32m     17\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# Move input tensors to GPU/CPU\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     21\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Don't forget the attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:50\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/Workspace/Project-NLP/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2766\u001B[0m, in \u001B[0;36mDataset.__getitems__\u001B[0;34m(self, keys)\u001B[0m\n\u001B[1;32m   2764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitems__\u001B[39m(\u001B[38;5;28mself\u001B[39m, keys: List) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List:\n\u001B[1;32m   2765\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2766\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getitem__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2767\u001B[0m     n_examples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch[\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(batch))])\n\u001B[1;32m   2768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [{col: array[i] \u001B[38;5;28;01mfor\u001B[39;00m col, array \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_examples)]\n",
      "Cell \u001B[0;32mIn[85], line 34\u001B[0m, in \u001B[0;36mParallelDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# Ensure that we're getting a single example from the lists\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m---> 34\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_mask[idx], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets[idx], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m     37\u001B[0m     }\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ebfabbf8245dfa66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
