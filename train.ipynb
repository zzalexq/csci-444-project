{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:08:52.682917Z",
     "start_time": "2024-11-26T09:08:48.741941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import csv  \n",
    "import stanza\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('zh') \n",
    "nlp = stanza.Pipeline('zh', processors='tokenize')\n",
    "\n",
    "# Get the set of stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(\n",
    "    {'cent', 'href=', 'http', 'says', 'told', 'year', 'ago', 'yesterday', 'since', 'last', 'past', 'next',\n",
    "     'said', 'almost', 'within', 'would', 'nearly', 'years', 'months', 'according', 'compared', 'go', 'also', \n",
    "     \"n't\"})  \n",
    "punctuation_set = set(punctuation)\n",
    "punctuation_set.update({\"’\", \"’\", '”', \"''\", \"“\", \"'s\", '--', 'b', '/b', '/strong', '–', '—'})"
   ],
   "id": "a90636181e9940f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.5MB/s]                    \n",
      "2024-11-26 01:08:49 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 01:08:49 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 01:08:49 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2024-11-26 01:08:50 INFO: File exists: /Users/vnnsnnt/stanza_resources/zh-hans/default.zip\n",
      "2024-11-26 01:08:52 INFO: Finished downloading models and saved to /Users/vnnsnnt/stanza_resources\n",
      "2024-11-26 01:08:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.2MB/s]                    \n",
      "2024-11-26 01:08:52 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 01:08:52 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 01:08:52 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsdsimp |\n",
      "=======================\n",
      "\n",
      "2024-11-26 01:08:52 INFO: Using device: cpu\n",
      "2024-11-26 01:08:52 INFO: Loading: tokenize\n",
      "2024-11-26 01:08:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:35:00.730157Z",
     "start_time": "2024-11-26T09:35:00.717380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Structures\n",
    "class ParallelSentence: \n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class AnchorWord:\n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class ParallelCorpus: \n",
    "    def __init__(self):\n",
    "        self.parallel_sentences = []\n",
    "        self.multi_grams_to_consider = []\n",
    "        self.anchor_words = []\n",
    "        \n",
    "    def load_parallel_sentences(self, data_source):\n",
    "        parallel_sentences = []\n",
    "        for file in os.listdir(data_source):\n",
    "            file_path = os.path.join(data_source, file)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as data_file:\n",
    "                reader = csv.reader(data_file, delimiter=';')\n",
    "                for row in reader:\n",
    "                    if len(row) < 7: continue   # escape bad data\n",
    "                    english_content = row[5]    # get english sentences\n",
    "                    chinese_content = row[6]    # get chinese sentences\n",
    "        \n",
    "                    # break apart sentence content by @ delimiter\n",
    "                    english_sentences = english_content.split('@')  \n",
    "                    chinese_sentences = chinese_content.split('@')\n",
    "                    \n",
    "                    for english_sentence, chinese_sentence in zip(english_sentences, chinese_sentences):\n",
    "                        clean_english_sentence = english_sentence.strip()\n",
    "                        \n",
    "                        # Process the Chinese sentence with Stanza\n",
    "                        doc = nlp(chinese_sentence)  \n",
    "                        chinese_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "                        clean_chinese_sentence = \" \".join(chinese_tokens)\n",
    "                        \n",
    "                        parallel_sentences.append(ParallelSentence(clean_english_sentence, clean_chinese_sentence))\n",
    "                        \n",
    "        self.parallel_sentences = parallel_sentences\n",
    "    \n",
    "    def generate_multi_grams(self):\n",
    "        bigrams = self.extract_ngram_counts(n=2).most_common()[:5000]\n",
    "        trigrams = self.extract_ngram_counts(n=3).most_common()[:3000]\n",
    "        quadgrams = self.extract_ngram_counts(n=4).most_common()[:1000]\n",
    "        \n",
    "        multi_grams_to_consider = set()\n",
    "        # Add multi-word terms from quad_grams_to_consider\n",
    "        for quad_gram in quadgrams:\n",
    "            multi_word_term = '_'.join(quad_gram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from trigrams_to_consider\n",
    "        for trigram in trigrams:\n",
    "            multi_word_term = '_'.join(trigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from bigrams_to_consider\n",
    "        for bigram in bigrams:\n",
    "            multi_word_term = '_'.join(bigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        self.multi_grams_to_consider = multi_grams_to_consider\n",
    "        \n",
    "    @staticmethod\n",
    "    def refactor_sentence_with_multiword_term(sentence, multi_word_terms):\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split(' ')\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            found = False\n",
    "            \n",
    "            # Check for quadgrams (4-word sequences)\n",
    "            for length in range(4, 1, -1):  # Check for quadgram to bigram\n",
    "                if i + length <= len(words):\n",
    "                    multi_word_candidate = '_'.join(words[i:i+length]).lower()\n",
    "                    if multi_word_candidate in multi_word_terms:\n",
    "                        # If a match is found, replace the words with the multi-word term\n",
    "                        modified_sentence.append(multi_word_candidate)\n",
    "                        i += length\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                # If no match is found, just add the word as is\n",
    "                modified_sentence.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        # Return the modified sentence as a string\n",
    "        return ' '.join(modified_sentence)\n",
    "    \n",
    "    def extract_ngram_counts(self, n):\n",
    "        ngram_counts = Counter()\n",
    "        for parallel_sentence in self.parallel_sentences:\n",
    "            tokens = nltk.word_tokenize(parallel_sentence.en)\n",
    "            # Filter out stopwords, punctuation, and numbers\n",
    "            filtered_tokens = [token.lower() for token in tokens \n",
    "                               if token.lower() not in stop_words \n",
    "                               and token not in punctuation_set \n",
    "                               and not token.isdigit()] \n",
    "    \n",
    "            # Generate n-grams for the filtered tokens\n",
    "            ngram_list = ngrams(filtered_tokens, n)\n",
    "            # Count the frequency of each n-gram\n",
    "            ngram_counts.update(ngram_list)\n",
    "        return ngram_counts\n",
    "    \n",
    "    def format_parallel_sentences_for_awesome_align(self):\n",
    "        with open(\"zhen.src-tgt\", \"w\") as f:\n",
    "            for parallel_sentence in self.parallel_sentences:\n",
    "                modified_sentence = self.refactor_sentence_with_multiword_term(parallel_sentence.en, self.multi_grams_to_consider)\n",
    "                f.write(f\"{modified_sentence} ||| {parallel_sentence.zh}\\n\")\n",
    "    \n",
    "    def build_anchor_words_from_awesome_align_output(self, alignments_path):\n",
    "        anchor_words = []\n",
    "        with open(alignments_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                alignment_pairs = line.strip().split(' ')\n",
    "                for index, pair in enumerate(alignment_pairs):\n",
    "                    en_entry, zh_entry = pair.split('<sep>')[0], pair.split('<sep>')[1]\n",
    "                    if en_entry not in self.multi_grams_to_consider: continue\n",
    "                    # Clean the English entry\n",
    "                    cleaned_en_entry = re.sub(r'[^a-zA-Z_]', '', en_entry)\n",
    "                    \n",
    "                    # Append only if conditions are met\n",
    "                    if cleaned_en_entry:\n",
    "                        if anchor_words and anchor_words[len(anchor_words)-1].en == cleaned_en_entry:\n",
    "                            if zh_entry not in anchor_words[len(anchor_words)-1].zh:\n",
    "                                anchor_words[len(anchor_words)-1].zh += zh_entry\n",
    "                        else:\n",
    "                            anchor_words.append(AnchorWord(cleaned_en_entry, zh_entry))\n",
    "                            \n",
    "        unique_anchors = set(AnchorWord(anchor.en, anchor.zh) for anchor in anchor_words)\n",
    "        \n",
    "        # Step 1: Count frequencies of `zh` entries for each `en`\n",
    "        anchor_freq = defaultdict(Counter)\n",
    "    \n",
    "        for anchor in unique_anchors:\n",
    "            anchor_freq[anchor.en][anchor.zh] += 1\n",
    "        \n",
    "        # Step 2: Select the most frequent `zh` entry for each `en`\n",
    "        filtered_alignments = []\n",
    "        for en, zh_counter in anchor_freq.items():\n",
    "            most_frequent_zh = zh_counter.most_common(1)[0][0]  # Get the most frequent `zh`\n",
    "            filtered_alignments.append(AnchorWord(en, most_frequent_zh))\n",
    "        \n",
    "        # Step 3: Sort alphabetically by `en`\n",
    "        sorted_filtered_anchors = sorted(filtered_alignments, key=lambda anchor: anchor.en)\n",
    "        \n",
    "        # Step 4: Write to file\n",
    "        with open('possible-anchors.txt', 'w') as file:\n",
    "            for alignment in sorted_filtered_anchors:\n",
    "                file.write(f\"{alignment.en} {alignment.zh}\\n\")\n",
    "    \n",
    "    def load_sorted_anchors(self, anchor_path):\n",
    "        anchors = []\n",
    "        with open(anchor_path, 'r') as file:\n",
    "            for line in file: \n",
    "                alignment = line.strip().split(' ')\n",
    "                en = alignment[0].replace('_', ' ')\n",
    "                zh = alignment[1] \n",
    "                anchors.append(AnchorWord(en, zh))\n",
    "        self.anchor_words = anchors"
   ],
   "id": "2e89c0453ef70e40",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:08:59.304542Z",
     "start_time": "2024-11-26T09:08:59.302445Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus = ParallelCorpus()  # Initialize Corpus Object",
   "id": "eaed50fde78b37",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:27:53.517543Z",
     "start_time": "2024-11-26T09:09:24.842651Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_parallel_sentences(data_source='./FTIE/')  # Load parallel sentences from data source",
   "id": "6ae262f0a64e3b4f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:33:49.811747Z",
     "start_time": "2024-11-26T09:32:11.186206Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.generate_multi_grams()  # Generate Multi grams e.g Asian Financial Crisis -> asian_financial_crisis",
   "id": "54d953742ad51578",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# parallel_corpus.format_parallel_sentences_for_awesome_align() # Format English Sentence With Multi Grams \n",
    "# Prepare a data source for awesome align \n",
    "\n",
    "# DATA_FILE=./zhen.src-tgt\n",
    "# MODEL_NAME_OR_PATH=./model_without_co\n",
    "# OUTPUT_FILE=./output.txt\n",
    "# OUTPUT_WORDS=./alignments.txt\n",
    "# OUTPUT_PROB=./alignments-prob.txt\n",
    "# \n",
    "# CUDA_VISIBLE_DEVICES=0 awesome-align \\\n",
    "#     --output_file=$OUTPUT_FILE \\\n",
    "#     --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "#     --data_file=$DATA_FILE \\\n",
    "#     --extraction 'softmax' \\\n",
    "#     --batch_size 32 \\\n",
    "#     --num_workers 0 \\\n",
    "#     --output_word_file=$OUTPUT_WORDS \\\n",
    "#     --output_prob_file=$OUTPUT_PROB "
   ],
   "id": "1f4cbeea37e3df17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:36:01.735614Z",
     "start_time": "2024-11-26T09:35:56.903747Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.build_anchor_words_from_awesome_align_output('./alignments.txt')    # Generate Possible Anchor Words",
   "id": "28a996a8a95010c7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:39:11.352235Z",
     "start_time": "2024-11-26T09:39:11.346183Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_sorted_anchors('./final_anchors.txt')  # Load Final and Verified Anchors",
   "id": "b7e9f9e66c29a192",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T10:22:42.564577Z",
     "start_time": "2024-11-26T09:58:33.731120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# How would the original model translate these anchor words? \n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the source and target languages\n",
    "tokenizer.src_lang = \"zh_CN\"  # Use 'zh_CN' for Simplified Chinese\n",
    "tokenizer.tgt_lang = \"en_XX\"  # 'en_XX' for English\n",
    "\n",
    "# Sample Chinese sentence\n",
    "with open(\"unmodified_zh-en-translated_anchor_words.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for index, pair in enumerate(parallel_corpus.anchor_words):\n",
    "        english_anchor = pair.en  # target\n",
    "        chinese_anchor = pair.zh  # source\n",
    "\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(chinese_anchor, return_tensors=\"pt\")\n",
    "        # Generate translation\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "        # Decode the translated tokens\n",
    "        translation = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Save the result in the text file\n",
    "        f.write(f\"{chinese_anchor}; {english_anchor}; {translation.lower()}\\n\")\n",
    "\n",
    "        if index % 100 == 0: \n",
    "            print(\"Done translating\", index, \"/\", len(parallel_corpus.parallel_sentences))"
   ],
   "id": "12b082515fe9f067",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done translating 0 / 255860\n",
      "Done translating 100 / 255860\n",
      "Done translating 200 / 255860\n",
      "Done translating 300 / 255860\n",
      "Done translating 400 / 255860\n",
      "Done translating 500 / 255860\n",
      "Done translating 600 / 255860\n",
      "Done translating 700 / 255860\n",
      "Done translating 800 / 255860\n",
      "Done translating 900 / 255860\n",
      "Done translating 1000 / 255860\n",
      "Done translating 1100 / 255860\n",
      "Done translating 1200 / 255860\n",
      "Done translating 1300 / 255860\n",
      "Done translating 1400 / 255860\n",
      "Done translating 1500 / 255860\n",
      "Done translating 1600 / 255860\n",
      "Done translating 1700 / 255860\n",
      "Done translating 1800 / 255860\n",
      "Done translating 1900 / 255860\n",
      "Done translating 2000 / 255860\n",
      "Done translating 2100 / 255860\n",
      "Done translating 2200 / 255860\n",
      "Done translating 2300 / 255860\n",
      "Done translating 2400 / 255860\n",
      "Done translating 2500 / 255860\n",
      "Done translating 2600 / 255860\n",
      "Done translating 2700 / 255860\n",
      "Done translating 2800 / 255860\n",
      "Done translating 2900 / 255860\n",
      "Done translating 3000 / 255860\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T10:29:22.168838Z",
     "start_time": "2024-11-26T10:29:22.162465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import Levenshtein\n",
    "anchor_count = len(parallel_corpus.anchor_words)\n",
    "perfect_match_count = 0\n",
    "matching_translations = 0\n",
    "with open(\"unmodified_zh-en-translated_anchor_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        items = line.split(';')\n",
    "        zh_anchor = items[0].strip()\n",
    "        en_anchor = items[1].strip()\n",
    "        translation = items[2].strip()\n",
    "        \n",
    "        if translation == en_anchor:\n",
    "            perfect_match_count += 1\n",
    "            matching_translations += 1\n",
    "        elif Levenshtein.distance(en_anchor, zh_anchor) <= 3:\n",
    "            matching_translations += 1\n",
    "\n",
    "print(\"Unmodified Accuracy on Chinese Anchor Words (zh->en):\", matching_translations / anchor_count)\n",
    "print(\"Perfect Match Count:\", perfect_match_count, \"out of\", anchor_count)"
   ],
   "id": "b85df343acb503d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmodified Accuracy on Chinese Anchor Words (zh->en): 0.3255584756898817\n",
      "Perfect Match Count: 987 out of 3044\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d1c5e0d1b326585"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
