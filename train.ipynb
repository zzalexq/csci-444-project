{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:08:52.682917Z",
     "start_time": "2024-11-26T09:08:48.741941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import csv  \n",
    "import stanza\n",
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re \n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('zh') \n",
    "nlp = stanza.Pipeline('zh', processors='tokenize')\n",
    "\n",
    "# Get the set of stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.update(\n",
    "    {'cent', 'href=', 'http', 'says', 'told', 'year', 'ago', 'yesterday', 'since', 'last', 'past', 'next',\n",
    "     'said', 'almost', 'within', 'would', 'nearly', 'years', 'months', 'according', 'compared', 'go', 'also', \n",
    "     \"n't\"})  \n",
    "punctuation_set = set(punctuation)\n",
    "punctuation_set.update({\"’\", \"’\", '”', \"''\", \"“\", \"'s\", '--', 'b', '/b', '/strong', '–', '—'})"
   ],
   "id": "a90636181e9940f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vnnsnnt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.5MB/s]                    \n",
      "2024-11-26 01:08:49 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 01:08:49 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 01:08:49 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2024-11-26 01:08:50 INFO: File exists: /Users/vnnsnnt/stanza_resources/zh-hans/default.zip\n",
      "2024-11-26 01:08:52 INFO: Finished downloading models and saved to /Users/vnnsnnt/stanza_resources\n",
      "2024-11-26 01:08:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.2MB/s]                    \n",
      "2024-11-26 01:08:52 INFO: Downloaded file to /Users/vnnsnnt/stanza_resources/resources.json\n",
      "2024-11-26 01:08:52 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-11-26 01:08:52 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsdsimp |\n",
      "=======================\n",
      "\n",
      "2024-11-26 01:08:52 INFO: Using device: cpu\n",
      "2024-11-26 01:08:52 INFO: Loading: tokenize\n",
      "2024-11-26 01:08:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:35:00.730157Z",
     "start_time": "2024-11-26T09:35:00.717380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Structures\n",
    "class ParallelSentence: \n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class AnchorWord:\n",
    "    def __init__(self, en, zh):\n",
    "        self.en = en\n",
    "        self.zh = zh\n",
    "        \n",
    "class ParallelCorpus: \n",
    "    def __init__(self):\n",
    "        self.parallel_sentences = []\n",
    "        self.multi_grams_to_consider = []\n",
    "        self.anchor_words = []\n",
    "        \n",
    "    def load_parallel_sentences(self, data_source):\n",
    "        parallel_sentences = []\n",
    "        for file in os.listdir(data_source):\n",
    "            file_path = os.path.join(data_source, file)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as data_file:\n",
    "                reader = csv.reader(data_file, delimiter=';')\n",
    "                for row in reader:\n",
    "                    if len(row) < 7: continue   # escape bad data\n",
    "                    english_content = row[5]    # get english sentences\n",
    "                    chinese_content = row[6]    # get chinese sentences\n",
    "        \n",
    "                    # break apart sentence content by @ delimiter\n",
    "                    english_sentences = english_content.split('@')  \n",
    "                    chinese_sentences = chinese_content.split('@')\n",
    "                    \n",
    "                    for english_sentence, chinese_sentence in zip(english_sentences, chinese_sentences):\n",
    "                        clean_english_sentence = english_sentence.strip()\n",
    "                        \n",
    "                        # Process the Chinese sentence with Stanza\n",
    "                        doc = nlp(chinese_sentence)  \n",
    "                        chinese_tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "                        clean_chinese_sentence = \" \".join(chinese_tokens)\n",
    "                        \n",
    "                        parallel_sentences.append(ParallelSentence(clean_english_sentence, clean_chinese_sentence))\n",
    "                        \n",
    "        self.parallel_sentences = parallel_sentences\n",
    "    \n",
    "    def generate_multi_grams(self):\n",
    "        bigrams = self.extract_ngram_counts(n=2).most_common()[:5000]\n",
    "        trigrams = self.extract_ngram_counts(n=3).most_common()[:3000]\n",
    "        quadgrams = self.extract_ngram_counts(n=4).most_common()[:1000]\n",
    "        \n",
    "        multi_grams_to_consider = set()\n",
    "        # Add multi-word terms from quad_grams_to_consider\n",
    "        for quad_gram in quadgrams:\n",
    "            multi_word_term = '_'.join(quad_gram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from trigrams_to_consider\n",
    "        for trigram in trigrams:\n",
    "            multi_word_term = '_'.join(trigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        # Add multi-word terms from bigrams_to_consider\n",
    "        for bigram in bigrams:\n",
    "            multi_word_term = '_'.join(bigram[0])\n",
    "            multi_grams_to_consider.add(multi_word_term)\n",
    "        \n",
    "        self.multi_grams_to_consider = multi_grams_to_consider\n",
    "        \n",
    "    @staticmethod\n",
    "    def refactor_sentence_with_multiword_term(sentence, multi_word_terms):\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split(' ')\n",
    "        modified_sentence = []\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            found = False\n",
    "            \n",
    "            # Check for quadgrams (4-word sequences)\n",
    "            for length in range(4, 1, -1):  # Check for quadgram to bigram\n",
    "                if i + length <= len(words):\n",
    "                    multi_word_candidate = '_'.join(words[i:i+length]).lower()\n",
    "                    if multi_word_candidate in multi_word_terms:\n",
    "                        # If a match is found, replace the words with the multi-word term\n",
    "                        modified_sentence.append(multi_word_candidate)\n",
    "                        i += length\n",
    "                        found = True\n",
    "                        break\n",
    "            \n",
    "            if not found:\n",
    "                # If no match is found, just add the word as is\n",
    "                modified_sentence.append(words[i])\n",
    "                i += 1\n",
    "\n",
    "        # Return the modified sentence as a string\n",
    "        return ' '.join(modified_sentence)\n",
    "    \n",
    "    def extract_ngram_counts(self, n):\n",
    "        ngram_counts = Counter()\n",
    "        for parallel_sentence in self.parallel_sentences:\n",
    "            tokens = nltk.word_tokenize(parallel_sentence.en)\n",
    "            # Filter out stopwords, punctuation, and numbers\n",
    "            filtered_tokens = [token.lower() for token in tokens \n",
    "                               if token.lower() not in stop_words \n",
    "                               and token not in punctuation_set \n",
    "                               and not token.isdigit()] \n",
    "    \n",
    "            # Generate n-grams for the filtered tokens\n",
    "            ngram_list = ngrams(filtered_tokens, n)\n",
    "            # Count the frequency of each n-gram\n",
    "            ngram_counts.update(ngram_list)\n",
    "        return ngram_counts\n",
    "    \n",
    "    def format_parallel_sentences_for_awesome_align(self):\n",
    "        with open(\"zhen.src-tgt\", \"w\") as f:\n",
    "            for parallel_sentence in self.parallel_sentences:\n",
    "                modified_sentence = self.refactor_sentence_with_multiword_term(parallel_sentence.en, self.multi_grams_to_consider)\n",
    "                f.write(f\"{modified_sentence} ||| {parallel_sentence.zh}\\n\")\n",
    "    \n",
    "    def build_anchor_words_from_awesome_align_output(self, alignments_path):\n",
    "        anchor_words = []\n",
    "        with open(alignments_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                alignment_pairs = line.strip().split(' ')\n",
    "                for index, pair in enumerate(alignment_pairs):\n",
    "                    en_entry, zh_entry = pair.split('<sep>')[0], pair.split('<sep>')[1]\n",
    "                    if en_entry not in self.multi_grams_to_consider: continue\n",
    "                    # Clean the English entry\n",
    "                    cleaned_en_entry = re.sub(r'[^a-zA-Z_]', '', en_entry)\n",
    "                    \n",
    "                    # Append only if conditions are met\n",
    "                    if cleaned_en_entry:\n",
    "                        if anchor_words and anchor_words[len(anchor_words)-1].en == cleaned_en_entry:\n",
    "                            if zh_entry not in anchor_words[len(anchor_words)-1].zh:\n",
    "                                anchor_words[len(anchor_words)-1].zh += zh_entry\n",
    "                        else:\n",
    "                            anchor_words.append(AnchorWord(cleaned_en_entry, zh_entry))\n",
    "                            \n",
    "        unique_anchors = set(AnchorWord(anchor.en, anchor.zh) for anchor in anchor_words)\n",
    "        \n",
    "        # Step 1: Count frequencies of `zh` entries for each `en`\n",
    "        anchor_freq = defaultdict(Counter)\n",
    "    \n",
    "        for anchor in unique_anchors:\n",
    "            anchor_freq[anchor.en][anchor.zh] += 1\n",
    "        \n",
    "        # Step 2: Select the most frequent `zh` entry for each `en`\n",
    "        filtered_alignments = []\n",
    "        for en, zh_counter in anchor_freq.items():\n",
    "            most_frequent_zh = zh_counter.most_common(1)[0][0]  # Get the most frequent `zh`\n",
    "            filtered_alignments.append(AnchorWord(en, most_frequent_zh))\n",
    "        \n",
    "        # Step 3: Sort alphabetically by `en`\n",
    "        sorted_filtered_anchors = sorted(filtered_alignments, key=lambda anchor: anchor.en)\n",
    "        \n",
    "        # Step 4: Write to file\n",
    "        with open('possible-anchors.txt', 'w') as file:\n",
    "            for alignment in sorted_filtered_anchors:\n",
    "                file.write(f\"{alignment.en} {alignment.zh}\\n\")\n",
    "    \n",
    "    def load_sorted_anchors(self, anchor_path):\n",
    "        anchors = []\n",
    "        with open(anchor_path, 'r') as file:\n",
    "            for line in file: \n",
    "                alignment = line.strip().split(' ')\n",
    "                en = alignment[0].replace('_', ' ')\n",
    "                zh = alignment[1] \n",
    "                anchors.append(AnchorWord(en, zh))\n",
    "        self.anchor_words = anchors"
   ],
   "id": "2e89c0453ef70e40",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:08:59.304542Z",
     "start_time": "2024-11-26T09:08:59.302445Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus = ParallelCorpus()  # Initialize Corpus Object",
   "id": "eaed50fde78b37",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:27:53.517543Z",
     "start_time": "2024-11-26T09:09:24.842651Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_parallel_sentences(data_source='./FTIE/')  # Load parallel sentences from data source",
   "id": "6ae262f0a64e3b4f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:33:49.811747Z",
     "start_time": "2024-11-26T09:32:11.186206Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.generate_multi_grams()  # Generate Multi grams e.g Asian Financial Crisis -> asian_financial_crisis",
   "id": "54d953742ad51578",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# parallel_corpus.format_parallel_sentences_for_awesome_align() # Format English Sentence With Multi Grams \n",
    "# Prepare a data source for awesome align \n",
    "\n",
    "# DATA_FILE=./zhen.src-tgt\n",
    "# MODEL_NAME_OR_PATH=./model_without_co\n",
    "# OUTPUT_FILE=./output.txt\n",
    "# OUTPUT_WORDS=./alignments.txt\n",
    "# OUTPUT_PROB=./alignments-prob.txt\n",
    "# \n",
    "# CUDA_VISIBLE_DEVICES=0 awesome-align \\\n",
    "#     --output_file=$OUTPUT_FILE \\\n",
    "#     --model_name_or_path=$MODEL_NAME_OR_PATH \\\n",
    "#     --data_file=$DATA_FILE \\\n",
    "#     --extraction 'softmax' \\\n",
    "#     --batch_size 32 \\\n",
    "#     --num_workers 0 \\\n",
    "#     --output_word_file=$OUTPUT_WORDS \\\n",
    "#     --output_prob_file=$OUTPUT_PROB "
   ],
   "id": "1f4cbeea37e3df17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:36:01.735614Z",
     "start_time": "2024-11-26T09:35:56.903747Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.build_anchor_words_from_awesome_align_output('./alignments.txt')    # Generate Possible Anchor Words",
   "id": "28a996a8a95010c7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T09:39:11.352235Z",
     "start_time": "2024-11-26T09:39:11.346183Z"
    }
   },
   "cell_type": "code",
   "source": "parallel_corpus.load_sorted_anchors('./final_anchors.txt')  # Load Final and Verified Anchors",
   "id": "b7e9f9e66c29a192",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "# \n",
    "# # Load mBART model and tokenizer\n",
    "# model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "# tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "# \n",
    "# # Set the source and target languages\n",
    "# tokenizer.src_lang = \"zh_CN\"  # Use 'zh_CN' for Simplified Chinese\n",
    "# tokenizer.tgt_lang = \"en_XX\"  # 'en_XX' for English\n",
    "# \n",
    "# # Sample Chinese sentence\n",
    "# with open(\"unmodified_zh-en-translated_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     print(\"Translating\", len(all_pairs), \"sentences\")\n",
    "#     for index, pair in enumerate(all_pairs):\n",
    "#         chinese_sentence = pair.zh  # source\n",
    "#         english_sentence = pair.en  # target\n",
    "#         \n",
    "#         # Tokenize the input text\n",
    "#         inputs = tokenizer(chinese_sentence, return_tensors=\"pt\")\n",
    "#         # Generate translation\n",
    "#         translated_tokens = model.generate(**inputs)\n",
    "#         # Decode the translated tokens\n",
    "#         translated_sentence = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "#         \n",
    "#         # Save the result in the text file\n",
    "#         f.write(f\"{chinese_sentence}; {english_sentence}; {translated_sentence}\\n\")\n",
    "#         \n",
    "#         if index % 100 == 0: \n",
    "#             print(\"Done with\", index, \"/\", len(all_pairs))"
   ],
   "id": "12b082515fe9f067",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
